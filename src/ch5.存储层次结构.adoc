== 存储层次结构

=== 存储技术及其优化

- SRAM技术

**SRAM技术（Static Random Access Memory）**是一种基于触发器（flip-flop）的存储技术，每个位的数据由一组互补的晶体管（通常为6个）构成。这种结构使得 SRAM 在保持数据时不需要周期性的刷新，因此其访问速度非常快。SRAM 的优点包括高速访问、低功耗（在无数据更改时）以及简单的控制电路，不需要刷新操作。但缺点在于单位存储密度较低，占用芯片面积较大，成本高昂，因此通常用于需要高速存取的缓存（如 CPU 的 L1、L2 缓存），而不适合作为大容量主存。

- DRAM技术

**DRAM技术（Dynamic Random Access Memory）**是一种基于电容的存储技术，每个位的数据存储在一个电容上，并通过一个晶体管进行访问。由于电容会随时间泄漏电荷，DRAM 需要定期刷新数据以保持其内容。DRAM 的主要优点是存储密度高、单位成本低，因此常被用于构建大容量主存（RAM）。然而，DRAM 的缺点是访问速度较慢，因为其需要刷新操作，此外，刷新还会消耗额外的能量并引入一定的延迟。

- SDRAM技术

**SDRAM技术（Synchronous Dynamic Random Access Memory）**是一种同步动态随机存储器，与 DRAM 的主要区别在于其与处理器的时钟信号同步工作。SDRAM 在时钟信号的上升沿或下降沿进行操作，能够通过流水线化访问技术，在一个周期内完成多组数据的读写操作，从而显著提高了存取速度。SDRAM 的优点包括访问速度更快，支持流水线操作，并且能够更好地与现代处理器的工作方式兼容。其缺点与 DRAM 类似，包括需要定期刷新、功耗较高，同时由于复杂度增加，设计和制造成本也较高。

- 闪存、磁盘

**闪存（Flash Memory）**是一种非易失性存储技术，利用电荷存储在浮动栅极晶体管中来保存数据，即使断电数据也不会丢失。闪存通常分为 NAND 闪存和 NOR 闪存两种类型，NAND 闪存以高存储密度和较低成本广泛用于存储卡、USB 驱动器和固态硬盘（SSD），而 NOR 闪存以较快的随机读取速度适用于嵌入式系统的代码存储。闪存的优点包括非易失性、低功耗、高存储密度和耐用性，特别是 NAND 闪存具有极高的写入和擦除耐久性。但缺点是擦写速度相对较慢，擦写操作需要以块为单位进行，随机写入性能较差，同时闪存的寿命有限，写入次数达到一定阈值后可能会导致单元失效。

**磁盘（Disk）**是传统的存储设备，主要包括硬盘驱动器（HDD）和光盘（如 CD、DVD）。磁盘通过机械部件（如旋转盘片和读写磁头）或光学原理来读取和写入数据。硬盘具有大容量和较低成本的优点，因此在长期数据存储、备份和非实时访问的场景中广泛应用。其缺点包括访问速度较慢（相较于闪存或 RAM），功耗较高，机械部件容易磨损，抗震性较差，且在频繁随机访问情况下性能表现不佳。光盘相较于硬盘，成本更低，便于长期存档，但容量较小且速度更慢。

- 图形数据RAM

**图形数据 RAM（Graphics Data RAM，GDRAM）**是一种专门为图形处理器（GPU）优化的高速存储技术，用于存储和访问图形数据，例如纹理、顶点和像素信息。现代图形数据 RAM 的代表包括 GDDR（Graphics Double Data Rate）和 HBM（High Bandwidth Memory）。其优点是高带宽和低延迟，能够满足图形处理所需的大量数据传输需求，同时支持并行处理，以加速渲染任务。但其缺点是成本较高，功耗较大，通常作为专用存储资源，仅在高性能 GPU 和图形处理应用中使用。

- 堆叠式或嵌入式DRAM


**堆叠式或嵌入式 DRAM（Stacked or Embedded DRAM）**是一种集成于芯片内部或与处理器直接堆叠在一起的动态随机存储器技术。它通过缩短存储器和处理器之间的数据传输距离，实现更快的访问速度和更低的能耗。嵌入式 DRAM（eDRAM）通常作为芯片上的缓存存储器，用于提高高性能计算、图形处理和嵌入式系统的效率，而堆叠式 DRAM 通过 3D 封装技术将 DRAM 堆叠到处理器上，常见于高端服务器和图形处理器中。
其优点包括访问延迟低、带宽高、能耗低以及与处理器集成度高，从而减轻主存储器的压力并提升系统整体性能。但缺点是制造工艺复杂，成本较高，同时容量相较于独立 DRAM 较小，因此多用于对性能和功耗要求较高的特定场景，而不是作为主存。

- 相变存储器技术

**相变存储器（Phase-Change Memory, PCM）**是一种非易失性存储技术，通过材料（通常为碲锑合金）在晶态和非晶态之间的相变来表示二进制数据。这种存储技术利用相变材料电阻值的差异来读取和写入数据，具有接近 DRAM 的访问速度，同时具备闪存的非易失性。PCM 的优点包括高读写速度、高存储密度、长写入寿命以及断电数据不丢失的特性。由于其无需定期刷新，可以显著减少能耗，特别适用于嵌入式设备、大数据存储和云计算领域。
PCM 的缺点在于制造工艺复杂且成本较高，同时写入操作能耗较大，相变材料在高频写入下可能退化，影响设备寿命。此外，PCM 的存储密度尚未完全赶上 NAND 闪存，因此在大容量应用中的经济性尚需进一步优化。

=== 存储层次结构的一般框架

缓存是位于处理器与存储器之间的速度更快的存储器。作用为将存储器中的数据提前放入速度更快的缓存中，处理器读写数据时先在缓存内查找，从而同时获得大容量与高速的存储器。

==== 块的位置

在较高存储层次结构中，块的放置可以使用一系列方案，从直接映射到组相联，再到全相联。实际上整个方案可以被认为是组相联方案的变体，其中组的数量和魅族中块的数量不相同：

[options="header"]
|====
|机制 | 组的数量 | 每组中块的数量

|直接映射
|cache中的块数
|1

|组相联
|stem:[\frac{cache中的块数}{相联度}]
|相联度（一般为2~16）

|全相联
|1
|cache中的块数

|====

增加相联度的好处是通常会降低失效率。失效率的改进来自于减少竞争同一位置而产生的失效。首先，来看能获得多少性能改进。最大的改进出现在直接映射变化到两路组相联时。随着cache容量的增加，相联度的提高对性能改进作用很小；这是因为大容量cache的总失效率较低，因此改善失效率的机会减少，并且由相联度引起的失效率的绝对改进明显减少。如前所述，相联度增加的潜在缺点是增加了代价和访问时间。

==== 块的识别

[options="header"]
|=======================
|机制|定位方法|需要比较的次数

|直接映射
|索引
|1

|组相联
|索引组，查找组中的元素
|相联度

.2+|全相联
|查找所有cache表项
|cache容量
|独立的查找表
|0

|=======================

在存储层次结构中，直接映射、组相联或全相联映射的选择取决于失效代价与相联度实现代价之间的权衡，包括时间和额外硬件开销。在片上包含二级cache允许实现更高的相联度。这是因为命中时间不再关键，设计者也不必依靠标准SRAM芯片来构建模块。除非容量很小，否则cache不使用全相联映射方式，其中比较器的成本并不是压倒性的，而绝对失效率的改进才是最明显的。

在虚拟存储系统中，页表是一张独立的映射表，它用来索引内存。除了表本身需要的存空间外，使用索引表还会引起额外的存储访问。使用全相联映射和额外的页表有以下几个原因：

- 全相联有其优越性，因为失效代价非常高。

- 全相联允许软件使用复杂的替换策略以降低失效率。

- 全相联很容易索引，不需要额外的硬件，也不需要进行查找。

因此，虚拟存储系统通常使用全相联。

组相联映射通常用于cache和TLB，访问时包括索引和组内查找。一些系统使用直接映cache，这是因为访问时间短并且实现简单。访问时间短是因为查找时不需要进行比较。这样的设计选择取决于很多实现细节，例如，cache是否集成在片上、实现cache的技术以及cache的访问时间对处理器周期时间的重要性。

==== 块的替换

当相联的cache发生失效时，我们必须决定要替换哪个块。在全相联的cache中，所有的块都是替换的候选者。如果cache是组相联的，则必须在一组的块中进行选择。当然，直接映射cache中的替换很容易，因为只有一个候选者。

在组相联或全相联的cache中有两种主要的替换策略：

- 随机：随机选择候选块，可能使用一些硬件辅助实现。

- 最近最少使用(LRU)：被替换的块是最久没有被使用过的块。

实际上，在相联度不低（典型的是两路到四路）的层次结构中实现LRU的代价太高，这是因为追踪记录使用信息的代价很高。即使对于四路组相联，LRU通常也是近似实现的，例如，追踪记录哪一对块是最近最少使用的（需要使用1位），然后追踪记录每对块中哪一块是最近最少使用的（每对需要使用1位）。

对于较大的相联度，LRU是近似的或使用随机替换策略。在cache中，替换算法由硬件实现，这意味着方案应该易于实现。随机替换算法用硬件很容易实现，对于两路组相联cache，随机替换的失效率比LRU替换策略的失效率高约1.l倍。随着cache容量变大，两种替换策略的失效率下降，并且绝对差异也变小。实际上，随机替换算法的性能有时可能比用硬件简单实现的近似LRU更好。

在虚拟存储中，LRU的一些形式都是近似的，因为当失效代价很大时，失效率的微小降低都很重要。通常提供参考位或其他等价的功能使操作系统更容易追踪记录一组最近使用较少的页。由于失效代价很高且相对不频繁发生，主要由软件来近似这项信息的做法是可行的。

==== 写入策略

[optinons="header"]
|====
|写策略|内容

|写穿透|处理器在进行写操作时同时向缓存与主存中写入，为避免写主存引起的长延时，还会增加写缓冲区。

|写返回|处理器进行写操作时只对缓存进行写入，并标记脏位。在这个块需要替换时才会写到主存中。此方法减少了对主存的频繁写入。
|====


任何存储层次结构的一个关键特性是如何处理写操作。我们已经看到两个基本选项：

- 写穿透：信息将写入cache中的块和存储层次结构中较低层的块（对cache而言是主存）

- 写返回：信息仅写人cache中的块。修改后的块只有在它被替换时才会写入层次结构中的较低层。

写返回和写穿透都有其各自的优点。写返回的主要优点如下：

- 处理器可以按cache而不是内存能接收的速率写单个的字。

- 块内的多次写操作只需对存储层次结构中的较低层进行一次写操作。

- 当写回块时，由于写一整个块，系统可以有效地利用高带宽传输。

写穿透具有以下优点：

- 失效比较简单，代价也比较小，这是因为不需要将块写回到存储层次结构中的较低层。

- 写穿透比写返回更容易实现，尽管实际上写穿透cache仍然需要写缓冲区。

在虚拟存储系统中，只有写返回策略才是实用的，这是因为写到存储层次结构较低层的延迟很大。尽管允许存储器的物理和逻辑宽度更宽，并对DRAM采用突发模式，处理器产生写操作的速率通常还是超过存储系统可以处理它们的速率。因此，现在最低一级的cache通常采用写回策略。

==== 3C模型：一种理解存储层次结构的直观模型

在本节中，我们将介绍一种模型，该模型可以很好地洞察存储层次结构中引起失效的原因，以及存储层次结构中的变化对失效的影响。我们将用cache来解释这些想法，尽管这些想法对其他层次也都直接适用。在此模型中，所有失效都被分为以下三类（3C模型）：

- 强制失效：对没有在cache中出现过的块进行第一次访问时产生的失效，也称为冷启动失效。

- 容量失效：cache无法包含程序执行期间所需的所有块而引起的失效。当某些块被替换出去，随后再被调入时，将发生容量失效。

- 冲突失效：在组相联或者直接映射cache中，很多块为了竞争同一个组导致的失效。冲突失效是直接映射或组相联cache中的失效，而在相同大小的全相联cache中不存在。这种cache失效也称为碰撞失效(collision miss)。下图显示了失效率如何按照引起的原因被分为三种。改变cache设计中的某一方面可以直接影响这些失效的原因。由于冲突失效来自对同一cache块的争用，因此提高相联度可减少冲突失效。但是，提高相联度可能会延长访问时间，从而降低整体性能。

TIP: p331图

简单地增大cache容量可以减少容量失效，实际上，多年来二级cache容量一直在稳步增长。当然，在增大cache的同时，我们也必须注意访问时间的增长，这可能导致整体性能降低。因此，尽管一级cache也在增大，但是非常缓慢。

由于强制失效是对块的第一次访问时产生的，因此，对cache系统来说，减少强制失效次数的主要方法是增加块大小。由于程序将由较少的cache块组成，因此这将减少对程序每一块都要访问一次时的总访问次数。如上所述，块容量增加太多可能对性能产生负面影响，因为失效代价会增加。

[options="header"]
|====
|设计变化 | 对失效率的影响 | 可能对性能产生的负面影响

|增加cache容量
|降低失效率
|可能延长访问时间

|增加相联度
|由于减少了冲突失效，降低了失效率
|可能延长访问时间

|增加块容量
|由于空间局部性，对很宽范围内变化的块的大小，降低了失效率
|增加失效损失，块太大还会增大失效率

|====

将失效分为3C是个有用的定性模型。在实际cache设计中，很多设计选择相互影响，改变一个cache特性通常会影响另一些失效率的组成部分。尽管存在这些缺点，但该模型仍是深入了解cache设计性能的有效方法。

=== 可靠的存储器层次

==== 失效的定义

假设有某种服务的需求，用户可以看到一个系统在两种分别有需求的服务的状态之间交替：

1. 服务完成：交付的服务与需求相符。

2. 服务中断：交付的服务与需求不同。

失效导致状态1到状态2的转换，而从状态2到状态l的转换过程被称为恢复。失效可能是永久性的或间歌性的，间歇性失效更为复杂。因为当系统在两种状态之间摇摆时，诊断更加困难。而永久性失效更容易诊断。

这里引出两个相关术语：可靠性和可用性。可靠性是一个系统能够持续提供用户需求的服务的度量，即从参考时刻到失效的时间间隔。因此，平均无故障时间（MTTF）是可靠性的度量方法。与之相关的一个术语是年度失效率（AFR），它是指给定MTTF一年内预期的器件失效百分比。当MTTF变大时，可能会产生误导性的结果，而AFR会带来更直观的结果。

服务中断使用平均修复时间（MTTR）来衡量。平均失效间隔时间stem:[(MTBF)=MTTF+MTTR]。尽管MTBF被广泛使用，MTTF却更加合适。然后，可用性是指系统正常工作时间在连续两次服务中断间隔时间中所占的比例：

[stem]
++++
可用性 = \frac{MTTF}{MTTF + MTTR}
++++

请注意，可靠性和可用性是可量化的，它们不仅仅是可信性的同义词。降低MTTR可以提高MTTF进而提高可用性。例如，用于故障检测、诊断和修复的工具可减少修复失效的时间。从而提高可用性。我们希望系统有很高的可用性。一种简写是“每年可用性中9的数量”。

为了提高MTTF，可以提高器件的质量，也可以设计能够在器件出现故障的情况下继续运行的系统。因此，由于器件的失效可能不会导致系统的失效，需要根据上下文对失效进行定义。为了明确二者的区别，用术语故障来表示器件的失效：以下是提高MTTF的三种方法：

1. 故障避免技术：通过合理构建系统来避免故障的出现。

2. 故障容忍技术：使用冗余技术，即使出现故障，仍然可以按照需求服务。

3. 故障预测技术：预测故障的出现和构建，从而允许在器件故障前进行替换。

==== 汉明编码

理查德·汉明（Richard Hamming）发明了一种广泛应用于存储器的冗余技术，并因此获得1968年的图灵奖。二进制数间的距离对于理解冗余码很有帮助。汉明距离是两个等长二进制数对应位置不同的位的数量。例如，011011和001111的距离为2。如果在一种编码中，码字之间的最小距离为2.且其中有1位错误，将会发生什么？这会将一个有效的码字转化为无效码字。因此，如果能够检测一个码字是否有效，就可以检测出1位的错误，称为1位错误检测编码。

汉明使用奇偶校验码进行错误检测。在奇偶校验码中，要计数一个字中1的个数是奇数还是偶数。当一个字被写人存储器时，奇偶校验位也被写入（1表示奇数，0表示偶数）。也就是说，N+1位字中1的个数永远是偶数。当读出该字时，奇偶校验位也一并读出并检查。如果计算出的校验码与存储的不匹配，则发生错误。

如果有2位同时出错，则1位奇偶校验位技术无法检测到错误，因为码字奇偶性不变。（实际上，1位奇偶校验可以检测任意奇数个错误，但是实际情况中，发生3位错误的概率远低于2位错误的概率。所以实际中1位奇偶校验码仅用于检测1位错误。）

当然，奇偶校验码不能纠正错误，汉明想要做到检错的同时又能纠错。如果码组中最小距离为3，那么任意发生1位错误的码字与其对应的正确码字的距离，要小于它与其他有效码字的距离。他想出了一个容易理解的将数据映射到距离3的码字，为纪念汉明，我们将这种方法称为汉明纠错码（ECC）。我们使用额外的奇偶校验位确定单个错误的位置。以下是计算汉明纠错码的步骤：

1. 从左到右由1开始依次编号，这与传统的从最右侧由0开始编号相反。

2. 将编号为2的整数幂的位标记为奇偶校验位（1, 2, 4, 8, 16, ...）。

3. 剩余其他位用于数据位（3, 5, 6, 7, 9, l0, 11, 12, l3, 14, l5, ...）。

4. 奇偶校验位的位置决定了其对应的数据位，如下所示：

- 校验位1（stem:[0001_2]）检查1, 3, 5, 7, 9, 11, ...位，这些位的编号最右一位为1（stem:[0001_2、0011_2、0101_2、0111_2、1001_2、1011_2, ...]）。

- 校验位2（stem:[0010_2]）检查2, 3, 6, 7, 10, 11, 14, 15, ...位，这些位的编号最右第二位为1。

- 校验位4（stem:[0100_2]）检查4~7, 12~15, 20~23, ...位，这些位的编号最右第三位为l。

- 校验位8（stem:[1000_2]）检查8~15, 24~31, 40~47, ...位，这些位的编号最右第四位为1。

	请注意，每个数据位都被至少两个奇偶校验位覆盖。

5. 设置奇偶校验位，为各组进行偶校验。

=== cache的性能评估与改进

CPU时间可被分成CPU用于执行程序的时间和CPU用来等待访存的时间。通常，假设cache命中的访问时间只是正常CPU执行时间的一部分。因此，

[stem]
++++
CPU时间 =（CPU执行的时钟周期数+等待存储访问的时钟周期数）\times 时钟周期
++++

假设等待存储访问的时钟周期数主要来自于cache失效，同时，限制后续的讨论只针对简单的存储系统模型。在真实的处理器中，读写操作产生的停顿十分复杂，准确的性能预测通常需要对处理器和存储系统进行非常详细的模拟。

等待存储访问的时钟周期数可以被定义为，读操作带来的停顿周期数加上写操作带来的停顿周期数：

[stem]
++++
等待存储访问的时钟周期数=读操作带来的停顿周期数+写操作带来的停顿周期数
++++
读操作带来的停顿周期数可以由每个程序的读操作数目、读操作失效率和读操作的失效代价来定义。

[stem]
++++
读操作带来的停顿周期数 = \frac{读操作数目}{程序} \times 读失效率 \times 读失效代价
++++

写操作要更复杂些。对于写穿透策略，有两个停顿的来源：一个是写失效，通常在连续写之前需要将数据块取回;另一个是写缓冲停顿，通常在写缓冲满时进行写操作会引发该停顿。因此，写操作带来的停顿周期数等于下面两部分的总和：

[stem]
++++
写操作带来的停顿周期数 = \frac{写操作数目}{程序} \times 写失效率 \times 写失效代价 + 写缓冲满时的停顿周期
++++

由于写缓冲停顿主要依赖于写操作的密集度，而不只是它的频度，不可能给出一个简单的计算此类停顿的等式。幸运的是，如果系统中有一个容量合理的写缓冲（例如，四个或者更多字），同时主存接收写请求的速度能够大于程序的平均写速度，写缓冲引起的停顿将会很少，几乎能够忽略。如果系统不能满足这些要求，那么这个设计可能不合理。设计者要么使用更深的写缓冲，要么使用写返回策略。

写返回策略也会额外增加停顿，主要来源于当数据块被替换并需要将其写回到主存时。

在大多数写穿透cache的结构中，读和写的失效代价是相同的（都是将数据块从内存取至cache所花的时间）。假设写缓冲停顿是可以忽略不计的，就可以使用失效率和失效代价来同时刻画读操作和写操作：

[stem]
++++
等待存储访间的时钟周期数 = \frac{访存操作数目}{程序} \times 写失效率 \times 写失效代价
++++
该公式也可以记作:

[stem]
++++
等待存储访问的时钟周期数 = \frac{指令数目}{程序} \times \frac{失效次数}{指令数目} \times 写失效代价
++++

==== cache的性能优化策略

1. 使用更为灵活的替换策略降低cache失效率

2. 在cache中查找数据块

3. 替换数据块的选择策略

4. 使用多级cache减少失效代价

5. 通过分块进行软件优化

=== 优化缓存性能

==== 采用小而简单的第一级缓存，缩短命中时间、降低功耗

提高时钟频率和降低功耗的双重压力推动了对第一级缓存大小的限制。类似地，使用较低级别的相联度可以缩短命中时间、降低功耗，不过这种权衡要比涉及缓存大小的权衡复杂一些。

缓存命中过程中的关键计时路径由3个步骤组成：

1. 使用地址中的索引确定标签存储器的地址

2. 将读取的标签值与地址进行比较

3. 如果缓存为组相联缓存，则设置多路选择器以选择正确的数据项。

直接映射的缓存可以将标签检查与数据传输重叠，有效缩短命中时间。此外，在采用低相联度时，由于减少了必须访问的缓存行，所以通常还可以降低功耗。

尽管随着新一代微处理器的出现，片上缓存的总量已经大幅增加，但由于大容量L1缓存对时钟频率的影响，Ll缓存大小最近的涨幅很小，甚至根本没有增长。在选择相联度时，另一个考虑因素是消除地址别名的可能性。

随着能耗变得至关重要，设计师们开始关注减少缓存访问所需能耗的方法。除了相联度之外，另一个决定缓存访问所需能耗的关键因素是缓存中块的数量，因为它决定了被访问的“行”的数量。设计师可以通过增加块大小（保持总缓存大小不变）来减少行数，但是这会增加缺失率，对于较小的L1缓存而言尤其如此。

另一种方法是将缓存分为多个存储体，这样一次访问将只激活缓存的一部分，也就是包含所需块的那个存储体。多体缓存主要用于增加缓存的带宽。多体缓存也会降低能耗，因为访问的缓存更少了。许多多核芯片中的L3缓存在逻辑上是统一的，但物理上是分散的，实际上就相当于一个多体缓存。一次请求实际上只会访问物理L3缓存中的一个缓存（也就是一个存储体），具体哪个缓存取决于请求中提供的地址。

在最近的设计中，有3种其他因素导致了在第一级缓存中使用更高的相联度。第一，许多处理器在访问缓存时至少需要两个时钟周期，因此命中时间较长可能不会产生很大影响。第二，为了将TLB排除在关键路径之外（TLB带来的延迟可能要大于高相联度导致的延迟），几乎所有L1缓存都应当是虚拟地址索引的。这就将缓存的大小限制为页面大小与相联度的乘积，因为只有页内的位才能用于索引。对于在完成地址变换之前对缓存进行索引的问题，还有其他解决方案，但提高相联度是最具吸引力的一种，它还有其他好处。第三，在引入多线程（参见第3章）之后，冲突缺失会增加，从而使提高相联度更有吸引力。

==== 采用路预测以缩短命中时间

这是另一种可以减少冲突缺失，同时又能保持直接映射缓存命中速度的方法。在路预测技术(way prediction）中，缓存中另外保存了一些位，用于预测下一次缓存访问中的路（即组中的块）。这种预测意味着要提前设定多路选择器，以选择所需要的块，并且在这个时钟周期中，在读取缓存数据的同时，只需要并行执行一次标签比较。如果缺失，则会在下一个时钟周期中再查看其他块，以找出匹配项。

在一个缓存的每个块中添加块预测位。根据这些位选定要在下一次缓存访问中尝试哪些块。如果预测正确，则缓存访问延迟就等于这一快速命中时间。如果预测错误，则尝试其他块，改变路预测器，并且延迟会增加一个时钟周期。模拟结果表明，对于一个两路组相联缓存，路预测准确率超过90%；对于四路组相联缓存，路预测准确率超过80%；指令缓存上的准确率高于对数据缓存。如果路预测能够至少快10%（这是非常可能的），路预测方法可以缩短两路组相联缓存的存储器平均访问时间。路预测于20世纪90年代中期首次用于MIPS R10000。它在使用两路组相联缓存的处理器中很流行，也用在几款使用四路组相联缓存的ARM处理器中。对于速度非常快的处理器，要将时延控制在一个周期是非常具有挑战性的，而这对于降低路预测失误代价非常关键。

还有一种扩展形式的路预测，它使用路预测位（本质上就是附加地址位）来判断实际访问的缓存块，也可以用来降低功耗。这种方法也可称为路选择（way selection)，当路预测正确时，它可以节省功耗，但在路预测错误时则会显著增加时间，这是因为需要重复进行访问，而不仅是重复标签匹配与选择过程。这种优化方法只有在低功耗处理器中才可能有意义。Inoue等人[1999]根据SPEC95基准测试进行估算，对于四路组相联缓存使用路选择方法，可以使指令缓存的平均访问时间增加1.04倍，数据缓存增加1.13倍，但与普通的四路组相联缓存相比，指令缓存的平均缓存功耗降为原来的0.28，数据缓存降为原来的0.35。路选择方法的一个重要缺点就是它增大了实现缓存访问流水化的难度。然而，随着能耗问题受关注度的增加，适时对缓存做低功耗处理的方案越来越有意义。

==== 通过缓存访问流水化和采用多体缓存来提升带宽

这类优化方法通过实现缓存访问的流水化，或者通过拓宽多体缓存，实现在每个时钟周期内进行多次访问，从而提高缓存的带宽。这类优化方法可同时用于实现提高指令吞吐率的超流水化和超标量技术。这些优化方法主要面向L1，这里的访问带宽限制了指令吞吐率。L2和L3级存中也会使用多个存储体，但主要是作为一种功耗管理技术。

L1缓存实现流水化后，可以采用更高的时钟频率，但代价是会增加延迟。例如，对于20世纪90年代中期的lntel Pentium处理器，指令缓存访问的流水线需要1个时钟周期；对于20世纪90年代中期至2000年的Pentium Pro到Pentium Ⅲ，需要2个时钟周期；对于2000年出现的Pentium4和现在的lntelCore i7，需要4个时钟周期。指令缓存访问的流水化实现上增加了流水线的段数，增加了分支预测错误的代价。相应地，数据缓存的流水化增加了从发出载入指令到使用数据之间的时钟周期数。如今，即使只是为了分开访问和命中检测这种简单的情况，所有处理器都会使用某种一级缓存流水化方法，而许多高速处理器则会采用三级或更多级缓存流水化方法。

指令缓存的流水化要比数据缓存容易一些，因为处理器可以依赖于高性能的分支预测来减轻延迟造成的影响。许多超标量处理器可以在一个时钟周期内发出和执行一个以上的存储器访问（允许一次载入或存储操作是常见情况，一些处理器允许进行多次载入）。为了在每个时钟周期内处理多个数据缓存访问，可以将缓存划分为独立的存储体，每个存储体为一次独立的访问提供支持。分体方式最初用于提高主存储器的性能，现在也用于现代DRAM芯片和缓存中。IntelCorei7的L1缓存中有4个存储体（可以支持在每个时钟周期内进行两次存储器访问）。

显然，当访问请求均匀分布在缓存组之间时，分体方式的效果最佳，所以将地址映射到存储体的方式会影响存储器系统的行为。一种简单有效的映射方式是将缓存块地址按顺序分散在这些存储体中，这种方式称为顺序交错(sequential interleaving)。例如，如果有4个存储体，0号存储体中的所有缓存块地址都是4的倍数，1号存储体中的所有缓存块地址都是模4余1，以此类推。采用分体方式还可以降低缓存和DRAM的功耗。

多体方式在L2缓存或L3缓存中也有应用，但原因不同。L2缓存中有多个存储体时，如果这些存储体没有冲突，那么可以同时处理多次L1缓存缺失——这是支持第四种优化方式非阻塞式缓存的关键能力。IntelCore i7中的L2缓存有8个存储体，而ARM Cortex处理器使用了具有1-4个存储体的L2缓存。前面曾经提到，采用分体方式还可以降低功耗。

==== 采用非阻塞缓存，以增加缓存带宽

对于允许乱序执行的流水化计算机，其处理器不必因为一次数据缓存缺失而停顿。例如，在等待数据缓存返回缺失数据时，处理器可以继续从指令缓存中取指。非阻塞缓存（nonblocking cache，或称无锁缓存，lockup-free cache）允许数据缓存在一次缺失期间继续提供缓存命中，从而进一步强化了这种方案的潜在优势。这种“缺失时仍然命中”优化方法在缺失期间非常有用，它虽然并没有真正忽略处理器的请求，但降低了实际的缺失代价。还有一种精巧而复杂的选择：如果能够重叠多个缺失，缓存就能进一步降低实际的缺失代价。这被称为“多次缺失时仍然命中”(hit under multiple miss)或者“缺失时缺失”(miss under miss)优化方法。只有当存储器系统可以为多次缺失提供服务时，第二种优化方法才有好处。大多数高性能处理器(比如Intel Core)通常支持这两种优化方法，而很多低端处理器仅在L2中提供了有限的非阻塞支持。

对非阻塞缓存进行性能评估时，真正的难度在于一次缓存缺失不一定会使处理器停顿。在这种情况下，很难判断一次缺失造成的影响，因此也就难以计算存储器平均访问时间。实际缺失代价并不等于这些缺失之和，而是等于处理器停顿的非重叠时间。非阻塞缓存的优势非常复杂，因为它取决于存在多次缺失时的缺失代价、存储器访问模式以及处理器在处理单次缺失时能够执行多少条指令。

通常，乱序处理器能够隐藏在L2缓存中命中但在L1数据缓存中缺失的大部分缺失代价，但无法隐藏更低层次缓存中缺失的大部分代价。在决定要支持多少个未处理缺失时，需要考虑多种因素，如下所述。

- 缺失流中的时间与空间局部性，它决定了一次缺失能否触发对低级缓存或对存储器的新访问操作。

- 对访问请求做出回应的存储器或缓存的带宽。

- 为了允许最低级别的缓存（这一级别的缺失时间是最长的）中出现更多的未处理缺失，需要在较高级别上支持至少同等数量的缺失，这是因为这些缺失必须在最高级别的缓存上启动。

- 存储器系统的延迟。

**实现非阻塞缓存**

尽管非阻塞缓存有提高性能的潜力，但其实现却并非易事。出现了两种类型的挑战：仲裁命中与缺失之间的冲突；跟踪尚未解决的缺失，以便知道何时可以处理载入或存储操作。

先考虑第一个问题。在阻塞式缓存中，缺失会导致处理器停顿，在缺失得到处理之前，不会发生对缓存的其他访问。而在非阻塞式缓存中，命中可能会与低一级存储器中返回的缺失发生冲突。如果允许存在多个尚未解决的缺失（几乎当前的所有处理器都允许这样做），那么缺失之间很可能会发生冲突。这些冲突必须得到解决，通常的做法是：首先为命中赋予比缺失更高的优先级，其次是在出现互相冲突的缺失时对其进行排序。

第二个问题的出现是因为我们需要跟踪多个尚未解决的缺失。在阻塞式缓存中，我们总是知道正在返回的缺失是哪个，因为只有一个缺失是尚未解决的。而在非阻塞式缓存中，这种情况就很少成立。乍看起来，你可能会认为这些缺失总是按顺序返回的，所以可以维护一个简单的队列，用以返回一个等待时间最长的缺失。但考虑一个发生在L1中的缺失。它在L2中可能发生一次命中，也可能造成一次缺失；如果L2是非阻塞式的，那么向L1返回缺失的顺序就未必与它们最初的发生顺序一致了。缓存访问时间不一致的多核系统及其他多处理器系统，也可能会引入这一复杂性。

在返回一个缺失时，处理器必须知道是哪个载入或存储操作导致了这一缺失，这样指令才能进行下去；它还必须知道应当将数据放到缓存中的什么位置（以及针对这个块的标签设置）。在当前的处理器中，这一信息保存在一组寄存器中，通常称为缺失状态处理寄存器（Miss StatusHandling Register，MSHR）。如果我们允许存在n个尚未解决的缺失，就会有n个MSHR，其中每一个中都保存了关于某一个缺失应当进入缓存中的什么位置，以及这一缺失的任意标签位的取值等信息，还包含了关于哪个载入或存储指令导致了这一缺失的信息。因此，在发生缺失时，我们分配一个MSHR来处理这个缺失，输入关于这一缺失的适当信息，并用MSHR的索引号来标记存储器请求。存储器系统在返回数据时使用该标签，从而使缓存系统能够将数据和标签信息传送给适当的缓存块，并向生成这一缺失的载人或存储操作发出“通知”，告诉它数据现在已经可用，它可以恢复执行了。非阻塞式缓存显然需要额外的逻辑处理，从而需要一点能耗。但很难精确评估它们的能耗开销，这是因为它们可能会缩短停顿时间，从而降低执行时间和相应的能耗。

除了上述问题之外，多处理器存储器系统，无论是单芯片的还是多芯片的，还必须处理与存储器一致性有关的复杂实现问题。另外，由于缓存缺失不再具有原子性（因为请求和响应是分离的，可能会在多个请求之间发生交错），所以存在出现死锁的可能性。

==== 利用关键字优先和提前重新执行以降低缺失代价

这种技术的基础是处理器通常一次仅需要缓存块中的一个字。这一策略显得“缺乏耐心”无须等待整个块载入完成，就可以发送请求的字并重新执行处理器。下面是两种具体策略。

- 关键字优先：首先从存储器中请求缺失的字，在其到达缓存之后立即发给处理器；让处理器能够在载入块中其他的字时继续执行。

- 提前重新执行：以正常顺序提取字，但只要块中的被请求字到达缓存，就立即将其发送给处理器，让处理器继续执行。

一般说来，这些技术只对使用大缓存块的设计有利。注意，在载入某个块中的其余内容时，缓存通常可以继续满足对其他块的访问请求。

不过，根据空间局部性原理，下一次访问很可能会指向这个块的其余内容。和非阻塞缓存一样，其缺失代价也不好计算。在采用关键字优先策略时，如果存在第二次请求，则实际缺失代价等于从本次访问开始到第二部分内容到达之前的非重叠时间。关键字优先和提前重新执行的好处取决于块的大小以及对块中尚未获取的部分进行另一次访问的可能性。例如，对于在i7 6700上运行的SPECimt2006（它采用了提前重新执行和关键字优先策略）来说，当有一个块发生缺失时，平均可以多做l次存储访问（平均1.23次，范围从0.5到3.0）。

==== 合并写缓冲区以降低缺失代价

因为所有存储内容都必须发送到层次结构的下一级，所以写直达缓存依赖于写缓冲区。即使是写回缓存，在替代一个块时也会使用一个简单的缓冲区。如果写缓冲区为空，则数据和整个地址被写到缓冲区中，从处理器的角度来看，写操作已经完成；在写缓冲区准备将字写入存储器时，处理器继续自己的工作。如果缓冲区中包含其他经过修改的块，则可以检查它们的地址，看看新数据的地址是否匹配写缓冲区某个条目的有效地址。如果匹配，则将新数据与这个条目合并在一起。这种优化方法称为写合并（writemerging）。Intel Core i7和其他许多处理器都采用了写合并方法。

如果缓冲区已满，而且没有匹配的地址，则缓存（和处理器）必须一直等到缓冲区中拥有空白条目为止。由于多字写入的速度通常快于每次只写入一个字的写操作，所以这种优化方法可以更高效地使用存储器。Skadron和Clark[1997]发现，即使是在一个合并4项的写缓冲区中，所生成的停顿也会导致5%-10%的性能损失。

==== 采用编译器优化以降低缺失率

前面介绍的技术都需要改变硬件。下面这种技术可以在不对硬件做任何改变的情况下降低缺失率。

这种神奇的效果来自软件优化——硬件设计人员最喜爱的解决方案！处理器与主存储器之间的性能差距越拉越大，已经促使编译器开发人员深入研究存储器的层次结构，以判断能否在程序编译时通过各种优化技术来提高性能。同样，研究包括两个方面：指令缓存缺失的性能改进和数据级存缺失的性能改进。下面介绍的优化技术在很多现代编译器中得到了应用。

1. 循环交换

	一些程序中存在嵌套循环，它们会以非连续顺序访问存储器中的数据。只要交换一下这些循环的嵌套顺序，就可以使程序代码按照数据的存储顺序来访问它们。如果缓存中无法容纳这些数组，这一技术可以通过改善空间局部性来减少缺失；通过重新排序，可以使缓存块中的数据在被替换之前，得到最大限度的利用。

2. 分块
	这种优化方法通过改善时间局部性来减少缓存缺失。我们还是要处理多个数组，其中有的数组按行访问，有的按列访问。由于在每个循环迭代中都用到了行与列，所以按行或按列来存储数组并不能解决问题[（按行存储称为行主序（rowmajor order），按列存储称为列主序（column major order）]。这种正交访问方式意味着在进行循环交换之类的转换操作之后，仍然有很大的改进空间。
	
	分块算法不是对一个数组的整行或整列进行操作，而是对其子矩阵（或称块）进行操作。其目的是在缓存中载入的数据被替换之前，最大限度地利用它。

==== 对指令和数据进行硬件预取，以降低缺失代价或缺失率

通过将执行过程与访存过程重叠，非阻塞级存能有效地降低缺失代价。另一种方法是在处理器真正需要某个数据之前，预先获取它们。指令和数据都可以预先提取，既可以直接放在缓存中，也可以放在一个访问速度快于主存储器的外部缓冲区中。

指令预取经常在缓存外部的硬件中完成。通常，处理器在一次缺失时提取两个块：被请求的块和下一个相邻块。被请求的块放在它返回时的指令缓存中，预取块被放在指令流缓冲区中。如果被请求的块当前存在于指令流级缓冲区中，则取消该缓存请求，从流缓冲区中读取这个块，并发出下一条预取请求。

类似方法可应用于数据访问[Jouppi.1990]。Palacharla 和Kessler[1994]研究了一组科学计算程序，并考查了多个可以处理指令或数据的流缓冲区。他们发现，对于一个具有两个64KiB四路组相联缓存的处理器（一个用于缓存指令，另一个用于缓存数据），8个流缓冲区可以捕获其所有缺失的50%-70%。

Intel core i7支持利用硬件预先提取到L1和L2中，最常见的预取情况是预取下一行。一些较早的lntel处理器使用更激进的硬件预取，但会导致某些应用程序的性能降低，一些高级用户会因此而关闭这一功能。

预取操作需要利用空闲的存储带宽，但如果它干扰了其他关键路径缺失内容的访问，反而会导致性能下降。在编译器的帮助下，可以减少无用预取。当预取操作正常执行时，它对功耗的影响可以忽略。如果预取的数据并未被用到或者替换了有用数据，预取操作会对功耗产生负面影响

==== 用编译器控制预取，以降低缺失代价或缺失率

硬件预取之外的另一种方法是，编译器插入预取指令，以便在处理器需要数据之前请求数据。共有以下两种预取。

- 寄存器预取将数据值载入一个寄存器。

- 缓存预取仅将数据载入缓存，而不载入寄存器。

这两种预取都可能触发异常，也可能不触发；也就是说，其地址可能会也可能不会导致虚拟地址错误异常和保护冲突异常。按照这一概念划分，普通的载入指令可被视为“故障性寄存器预取指令”。如果一次预取可能导致异常，那么就把它转为空操作，空操作不会触发缺页错误。这样的非故障性预取是我们想要的。

最有效的预取对程序来说是“语义上不可见的”：它不会改变寄存器和存储器的内容，也不会导致虚拟存储器错误。今天的大多数处理器提供非故障性缓存预取能力。本节采用非故障性缓存预取，也称为非绑定（nonbinding）预取。

只有当处理器在预取数据时能够继续工作的情况下，预取才有意义；也就是说，缓存在等待返回预取数据时不会停顿，而是继续提供指令和数据。可以想见，这些计算机的数据缓存通常是非阻塞性的。

与硬件控制的预取操作类似，这里的目标也是将执行过程与数据预取过程重叠。循环是重要的预取优化目标，因为它们本身很适合进行预取优化。如果缺失代价很小，编译器只需将循环展开一两次，在执行时调度这些预取操作。如果缺失代价很大，它会使用软件流水线或者将循环展开多次，以预先提取数据，供后续迭代使用。

不过，发出预取指令会带来指令开销，所以编译器必须确保这些开销不会大于所得到的好处。如果程序能够将注意力放在那些可能导致缓存缺失的访问上，就可以避免不必要的预取操作，同时大大缩短存储器平均访问时间。

尽管数组优化很好理解，但现代程序更倾向于使用指针。Luk和Mowry [1999]已经证明。基于编译器的预取优化有时也可以扩展到指针。在10个使用递归数据结构的程序中，在访问一个节点时预取所有指针，可以使一半程序的性能提高4%-31%，而其他程序的性能变化不超过原性能的2%。问题是预取是否针对级存中已经存在的数据，以及预取是否执行得足够早，以便数据在需要时及时到达。

许多处理器支持缓存预取指令，高端处理器（比如 Intel Core i7）还经常在硬件中完成某种自动预取。

==== 使用HBM扩展存储器层次结构

HBM 封装技术中封装的存储器容量，难以满足服务器中的大多数通用处理器对存储器的需求，所以人们建议使用与计算芯片封装在一起的DRAM来构建大容量的L4缓存。随着128MiB至1GiB以上HBM技术的出现，L4缓存的容量要远大于目前的片上L3缓存容量。使用如此之大的基于DRAM的缓存会带来一个问题：缓存标签放在哪里？这取决于标签的数量。假定我们使用的块大小为64B，那么lGiB的L4缓存需要96MiB的标签，远多于CPU上缓存中的静态存储器数量。将块大小增大至4KiB，会使标签存储急剧缩减至256000项，也就是总存储量小于1MiB。如果下一代多核处理器中的L3缓存达到4-16MiB或更多，那么这样的标签存储是可能接受的。但这样大的块大小有两个重要问题。

首先，如果许多块中的内容都不会用到，那么缓存的使用效率可能会比较低下；这称为碎片化问题，它也出现于虚拟存储器系统中。此外，如果许多数据都是没用的，那么传送这样大的数据块也是效率低下的。其次，由于数据块比较大，所以DRAM缓存中保存的不同数据块的数目就要少得多了，这样会导致更多的缺失，尤其是冲突缺失和一致性缺失。

第一个问题的部分解决方法是增加子块。子块允许一个缓存行中只有部分数据是有效的，当发生缺失时，可以只获取其中有效的子块。但对于解决第二个问题，子块无能为力。

使用较小的数据块时，标签存储是一个主要缺陷。该问题有一个可能有效的解决方案，就是直接把HBM作为L4缓存的标签存储到HBM中。乍看起来，这似乎是不可行的，因为每访问一次L4缓存都需要访问两次DRAM：一次用于标签，一次用于数据本身。由于随机 DRAM访问的时间较长，通常为100或更多个处理器时钟周期，所以这种方法曾经被放弃。Loh和Hill（2011）为这一问题提出了一种更聪明的解决方案：将标签和数据放在HBM SDRAM中的同一行中。尽管打开这个行（还有最后关闭这个行）需要大量时间，但访问同一行的不同部分所带来的CAS延迟，大约是访问一个新行所需时间的三分之一。因此，我们可以先访问这个块的标签部分，如果命中，则使用一次列访问来选择正确的字。Loh和Hi(L-H)还建议在设计L4HBM缓存的组织形式时，每个SDRAM行都包含一组标签（位于数据块的头部）和29个数据段，组成一个29路组相联缓存。在访问L4级存时，打开一个合适的行，并读取标签；一次命中只需要再增加一次列访问就可以获得匹配数据。

Qureshi和Loh(2012)年提出了一种称为熔合缓存（alloy cache）的改进方法，它可以缩短命中时间。熔合缓存将标签和数据融在一起，并使用一种直接映射的缓存结构。这一改进通过直接对HBM缓存进行索引，并对标签和数据进行突发传输，使L4缓存访问时间缩短到一个HBM周期。

=== 提高存储器系统的可靠性

大型缓存和主存储器显著增加了制造过程和操作过程中动态发生错误的可能性。由电路变化引起的可重复的错误称为硬错误(hard error）或永久性故障(permanent fault）。硬错误可能发生在制造过程中，也可能发生在操作过程中的电路更改中（例如，在多次写入之后闪存单元发生故障）。所有DRAM、闪存和大多数SRAM在制造时都留有备用行，因此通过编程用备用行替换有缺陷的行可以解决少量的制造缺陷。动态错误是指在电路不改变的前提下存储单元内容发生改变的情况，被称为软错误（sof error)或瞬态故障（transient fault)。

动态错误可以使用奇偶校验检测，可以使用纠错码（ECC）检测和纠正。因为指令缓存是只读的，所以用奇偶校验就够了。在更大型的数据缓存和主存储器中，则使用ECC技术来检测和纠正错误。奇偶校验只需要占用一个数据位就可以检测一系列数据位中的一个错误。由于无法使用奇偶校验来检测多位错误，所以必须限制用奇偶校验提供保护的位数。典型的比例是每8个数据位使用一个奇偶校验位。ECC可以检测两个错误并纠正一个错误，代价是每64个数据位占用8位的开销。

在规模庞大的系统中，出现多个错误乃至单个存储芯片完全失效的概率非常大。IBM引入了Chipkill来解决这一问题。许多大规模系统使用这一技术，比如IBM和SUN服务器以及GoogleClusters。(Intel将其自己的版本命名为SDDC。）Chipkill在本质上类似于磁盘中使用的RAID方法，它分散数据和ECC信息，以便在单个存储芯片完全失效时，可以从其余存储芯片中重构丢失数据。根据IBM的分析，假定有一台具有10000个处理器的服务器（每个处理器有4GiB存储器)，在3年的运行中出现不可恢复错误的数目如下所示。

- 仅采用奇偶校验位——大约90000个，或者说每17分钟一个不可恢复（或未检测到）的故障。

- 仅采用ECC——大约3500个，或者说大约每7.5小时一个不可恢复（或未检测到）的故障。

- Chipkill——大约每2个月一个不可恢复（或未检测到）的故障。

看待这个问题的另一种方法是，在实现与Chipkill相同错误率的同时，求出其他两种方式可以保护的最大服务器数目（每个服务器拥有4GiB存储器）。采用奇偶校验位方法时，即使是一台仅包括一个处理器的服务器，其不可恢复的错误率也要高于由10000台服务器组成、受Chipkil保护的系统。采用ECC方法时，一个包含17台服务器的系统与一个包含10000台服务器的Chipkill系统的故障率大体相同。因此，对于仓库级计算机中的50 000-100 000台服务器来说，需要采用Chipkill方法。

=== 虚拟存储器和虚拟机

- 要实现多个程序同时运行，共享内存空间。将内存划分并通过页表将程序与真实的物理地址相联系，这样在程序看来是自己独占内存。

- 虚拟机可以使多个用户共享同一台计算机，且用户本身感知不到其他用户的存在。虚拟机监视器（VMM）决定如何将虚拟资源映射到物理资源上。

主存可以为通常由磁盘实现的辅助存储充当“cache”。这种技术被称为虚拟存储。从历史上看，提出虚拟存储的主要动机有两个：允许在多个程序之间高效安全地共享内存，例如云计算的多个虚拟机所需的内存，以及消除小而受限的主存容量对程序设计造成的影响。50年后，第一条变成主要设计动机。

当然，为了允许多个虚拟机共享内存，必须保护虚拟机免受其他虚拟机影响，确保程序只读写分配给它的那部分主存。主存只需存储众多虚拟机中活跃的部分，就像cache只存放一个程序的活跃部分一样。因此，局部性原理支持虚拟存储和cache，虚拟存储允许我们高效地共享处理器以及主存。

在编译虚拟机时，无法知道哪些虚拟机将与其他虚拟机共享存储。事实上，共享存储的虚拟机在运行时会动态变化。由于这种动态的交互作用，我们希望将每个程序都编译到它自己的地址空间中——只有这个程序能访问的一系列存储位置。虚拟存储实现了将程序地址空间转换为物理地址。这种地址转换处理加强了各个程序地址空间之间的保护。

虚拟内存的第二个动机是允许单用户程序使用超出内存容量的内存。以前，如果一个程序对于存储器来说太大，程序员应该调整它。程序员将程序划分成很多段，并将这些段标记为互斥的。这些程序段在执行时由用户程序控制载入或换出，程序员确保程序在任何时候都不会访问未载入的程序段，并且载入的程序段不会超过内存的总容量。传统的程序段被组织为模块，每个模块都包含代码和数据。不同模块之间的过程调用将导致一个模块覆盖掉另一个模块。

可以想象，这种责任对程序员来说是很大的负担。虚拟存储的发明就是为了将程序员从这些困境中解脱出来，它自动管理由主存（有时称为物理内存，以区分虚拟存储）和辅助存储所代表的两级存储层次结构。

虽然虚拟存储和cache的工作原理相同，但不同的历史根源导致它们使用的术语不同。虚拟存储块被称为页，虚拟存储失效被称为缺页失效。在虚拟存储中，处理器产生一个虚拟地址，该地址通过软硬件转换为一个物理地址，物理地址可访问主存。这个过程被称为地址映射或地址转换：如今，由虚拟存储控制的两级存储层次结构通常是个人移动设备中的DRAM和闪存，在服务器中是DRAM和磁盘。如果我们使用图书馆类比，可以将虚拟地址视为书名，将物理地址视为图书馆中该书的位置，它可能是图书馆的索书号。

虚拟内存还通过重定位简化了执行时程序的载入。在用地址访问存储之前，重定位将程序使用的虚拟地址映射到不同的物理地址。重定位允许将程序载入主存中的任何位置。此外，现今所有的虚拟存储系统都将程序重定位成一组固定大小的块（页），从而不需要寻找连续内存块来放置程序：相反，操作系统只需要在主存中找到足够数量的页。

在虚拟存储中，地址被划分为虚拟页号和页内偏移。物理页号构成物理地址的高位部分。页内偏移不变，它构成物理地址的低位部分。页内偏移的位数决定页的大小。虚拟地址可寻址的页数与物理地址可寻址的页数可以不同。拥有比物理页更多数量的虚拟页是一个没有容量限制的虚拟存储的基础。

缺页失效的高成本是许多设计选择虚拟存储系统的原因。磁盘的缺页失效处理需要数百万个时钟周期。这种巨大的失效代价（主要由获得标准页大小的第一个字所花费的时间来确定）导致了设计虚拟存储系统时的几个关键决策：

- 页应该足够大以分摊长访问时间。目前典型的页大小从4KiB到64KiB不等。支持32KiB和64KiB页的新型桌面和服务器正在研发，但是新的嵌入式系统正朝另一个方向发展，页大小为lKiB。

- 能降低缺页失效率的组织结构很有吸引力。这里使用的主要技术是允许存储中的页以全相联方式放置。

- 缺页失效可以由软件处理，因为与磁盘访问时间相比，这样的开销将很小。此外，软件可以用巧妙的算法来选择如何放置页面，只要失效率减少很小一部分就足以弥补算法的开销。

- 写穿透策略对于虚拟存储不合适，因为写人时间过长。相反，虚拟存储系统采用写回策略。

==== 页的存放、查找、失效

由于缺页失效的代价非常高，设计人员通过优化页的放置来降低缺页失效频率。如果允许一个虚拟页映射到任何一个物理页，那么在发生缺页失效时，操作系统可以选择任意一个页进行替换。例如，操作系统可以使用复杂的算法和复杂的数据结构来跟踪页面使用情况，来选择在较长一段时间内不会被用到的页。使用先进而灵活的替换策略降低了缺页失效率，并简化了全相联方式下页的放置。

全相联映射的困难在于项的定位，因为它可以在较高存储层次结构中的任何位置。全部进行检索是不切实际的。在虚拟存储系统中，我们使用一个索引主存的表来定位页；这个结构称为页表，它被存在主存中。页表使用虚拟地址中的页号作为索引，找到相应的物理页号。每个程序都有自己的页表，它将程序的虚拟地址空间映射到主存。在图书馆类比中，页表对应于书名和图书馆位置之间的映射。就像卡片目录可能包含学校中另一个图书馆中书的表项，而不仅仅是本地的分馆，我们将看到该页表也可能包含不在内存中的页的表项。为了表明页表在内存中的位置，硬件包含一个指向页表首地址的寄存器，我们称之为页表寄存器。现在假定页表存在存储器中一个固定的连续区域中。

由于页表包含了每个可能的虚拟页的映射，因此不需要标签。在cache术语中，索引是用来访问页表的，这里由整个块地址即虚拟页号组成。

如果虚拟页的有效位为无效，则会发生缺页失效。操作系统获得控制。这种控制的转移通过例外机制完成。一旦操作系统得到控制、它必须在存储层次结构的下一级（通常是闪存或磁盘）中找到该页，并确定将请求的页放在主存中的什么位置。

虚拟地址本身并不会立即告诉我们该页在辅助存储中的位置。回到图书馆的类比，我们无法仅依靠书名就找到图书的具体位置。而是按目录查找，获得书在书架上的位置信息，比如说图书馆的索引书号。同样，在虚拟存储系统中，我们必须跟踪记录虚拟地址空间的每一页在辅助存储中的位置。

由于我们无法提前获知存储器中的某一页什么时候将被替换出去，因此操作系统通常会在创建进程时为所有页面在闪存或磁盘上创建空间。这个空间被称为交换区。那时，它也会创建一个数据结构来记录每个虚拟页在磁盘上的存储位置。该数据结构可以是页表的一部分，或者可以是具有与页表相同索引方式的辅助数据结构。

操作系统还会创建一个数据结构用于跟踪记录使用每个物理地址的是哪些进程和哪些虚拟地址。发生缺页失效时，如果内存中的所有页都正在使用，则操作系统必须选择一页进行替换。因为我们希望尽量减少缺页失效次数，所以大多数操作系统选择它们认为近期内不会使用的页进行替换。使用过去的信息预测未来，操作系统遵循最近最少优用（LRU）替换策略。操作系统查找最近最少使用的页，假定某一页在很长一段时间都没有被访问，那么该页再被访问的可能性比最近经常访问的页的可能性要小。被替换的页被写到辅助存储器中的交换区。

==== 快速地址变换技术（TLB）

由于页表存储在主存中，因此程序每个访存请求至少需要两次访存：第一次访存获得物理地址，第二次访存获得数据。提高访问性能的关键在于页表的访问局部性。当使用虚拟页号进行地址转换时，它可能很快再次被用到，因为对该页中字的引用同时具有时间局部性和空间局部性。

因此，现代处理器包含一个特殊的cache以追踪记录最近使用过的地址转换。这个特殊的地址转换cache通常被称为快表（TLB）。TLB就相当于记录目录中的一些书的位置的小纸片：我们在纸片上记录一些书的位置，并且将小纸片当成图书馆索书号的cache，这样就不用一直在整个目录中搜索了。

TLB中的每个标签项保存虚拟页号的一部分，每个数据项保存一个物理号。因为每次引用都访问TLB而不是页表，所以TLB需要包括其他状态位，例如脏位和用位。TLB也适用于多级页表。TLB从最后一级页表中载入物理地址和保护标签即可。

每次引用时，在TLB中查找虚拟页号。如果命中，则使用物理页号来形成地址，相应的引用位被置位。如果处理器要执行写操作，那么脏位也会被置位。如果TLB发生失效，我们必须确定是缺页失效或只是TLB失效。如果该页在内存中，TLB失效表明缺少该地址转换。在这种情况下，处理器可以将（最后一级）页表中的地址转换加载到TLB中，并重新访问来处理失效。如果该页不在内存中，那么TLB失效意味着真正的缺页失效。在这种情况下，处理器调用操作系统的例外处理。由于TLB的项数比主存中的页数少得多，TLB失效比缺页失效更频繁。

TLB失效可以通过硬件或软件处理。实际上，两种方法之间几乎没有性能差异，因为它们的基本操作相同。

发生TLB失效并从页表中检索到失效的地址转换后，需要选择要替换的TLB表项。由于TLB表项包含引用位和脏位，所以当替换某一TLB表项时，需要将这些位复制回对应的页表项。这些位是TLB表项中唯一可修改的部分。使用写回策略——在失效时将这些表项写回而不是任何写操作都写回——是非常有效的，因为我们期望TLB失效率很低。一些系统使用其他技术来粗略估计引用位和脏位，这样在失效时无须写入TLB，只需载入新的表项。

TLB中相联度的设置非常多样。一些系统使用小的全相联TLB，因为全相联映射的失效率较低；同时由于TLB很小，全相联映射的成本也不是太高。其他一些系统使用容量大的TLB，通常其相联度较小。在全相联映射的方式下，由于硬件实现LRU方案成本太高，替换表项的选择就很复杂。此外，由于TLB失效比缺页失效更频繁，因此需要用较低的代价来处理，而不能像缺页失效那样选择一个开销大的软件算法。所以很多系统都支持随机选择替换表项。

TIP: 体系结构p93

==== 通过虚拟存储器提供保护

页式虚拟存储器（包括缓存页表条目的TLB）是避免进程相互影响的主要机制。多道程序设计（multiprogramming，几个同时运行的程序共享一台计算机的资源）需要在各个程序之间提供保护和共享，从而产生了进程（process）的概念。进程就是一个程序呼吸的空气、生存的空间——也就是一个正在运行的程序加上继续运行它所需的全部状态。在任意时刻，必须能够从一个进程切换到另一个进程。这种交换被称为进程切换(process switch)或上下文切换(context switch)。

操作系统和体系结构联合起来就能使进程共享硬件而不会相互干扰。为此，在运行一个用户进程时，体系结构必须限制用户进程能够访问的资源，但要允许操作系统进程访问更多资源。体系结构至少要做到以下几点。

1. 提供至少两种模式，指出正在运行的进程是用户进程还是操作系统进程。后者有时被称为内核（kemel）进程或管理（supervisor）进程。

2. 提供用户进程可以使用但不能写入的处理器状态的一部分。这种状态包括用户/管理模式位、异常启用/禁用位和存储器访问权限信息。之所以禁止用户写入这些状态信息，是因为如果用户可以授予自己管理员权限、禁用异常或者改变存储器访问权限，操作系统就不能控制用户进程了。

3. 提供处理器借以从用户模式转为管理模式及反向转换的机制。前一种转换通常通过系统调用（system call）完成，使用一种特殊指令将控制传递到管理代码空间的一个专用位置。保存系统调用时刻的程序计数器，处理器转入管理模式。返回用户模式的过程类似于子程序返回过程，恢复到先前的用户/管理模式。

4. 提供限制存储器访问的机制，这样在上下文切换时不需要将进程切换到磁盘就能保护该进程的存储器状态。固定大小的页面（通常长4KiB或8KiB）通过一个页表由虚拟地址空间映射到物理地址空间。这些保护性限制就包含在每个页表项中。保护性限制可以决定一个用户进程能否读取写入这个页而，以及能否从这个页而执行代码。此外，如果一个进程没有包含在页表中，那它就既不能读取也不能写人一个页面。由于只有操作系统才能更新页表，所以分页机制提供了全面的访问保护。

分页虚拟存储器意味着每次存储器访问在逻辑上都要花费至少两倍的时间，一次存储器访问用于获取物理地址，第二次访问用于获取数据。这种操作成本太高了。解决方案是依靠局部性原理。如果这些访问具有局部性，那么访问操作的地址变换(address translation）也肯定具有局部性。只要将这些地址变换放在一个特殊的缓存中，存储器访问就很少需要第二次访问操作来变换地址了。这种特殊的地址变换级存被称为变换旁路缓冲区（TLB）。

TLB条目类似于缓存条目，其中的标签保存虚拟地址的一部分，数据部分保存物理页地址保护字段、有效位，通常还有一个使用位和一个脏位（diry bit）。操作系统在改变这些位时，首先改变页表中的值，然后使相应的TLB项失效。当从页表重新载入这个条目时，TLB即获得这些位的准确副本。

==== 通过虚拟机提供保护

虚拟机最早是在20世纪60年代后期提出的，多年以来一直是大型机计算的重要组成部分。尽管它在20世纪80年代和90年代的单用户计算机领域被忽视，但近来再度得到广泛关注，原因如下：

- 隔离与安全在现代系统中的重要性提高了；

- 标准操作系统的安全性和可靠性出现了问题；
- 许多不相关的用户（比如一个数据中心或云中的用户）会共享同一计算机；

- 处理器原始速度的飞速增长，使虚拟机的开销更容易被人接受。

最广义的虚拟机定义基本上包括了所有提供标准软件接口的仿真方法，比如JavaVM。我们感兴趣的是那些在二进制指令集体系结构（ISA)级别提供完整系统级环境的虚拟机。最常见的情况是，VM支持的ISA与底层硬件相同。然而，VM也有可能支持不同的ISA，在ISA之间迁移时经常采用这种方法，这样在迁移到新ISA之前，软件仍能在原ISA上使用。在我们重点关注的虚拟机中，VM使用的ISA与其底层硬件相匹配。这种虚拟机称为（操作）系统虚拟机(system virtual machine)，IBM VM/370、VMware ESX Server 和Xen都属于此类虚拟机。它们给人一种错觉：虚拟机用户拥有整台计算机，包括操作系统的副本。一台计算机可以运行多个虚拟机，可以支持多种操作系统。在传统平台上，一个操作系统“拥有”所有硬件资源但在使用虚拟机时，多个操作系统一起共享硬件资源。

为虚拟机提供支持的软件称为虚拟机监视器（VMM）或管理程序（bypervisor），VMM是虚拟机技术的核心。底层硬件平台称为宿主机（host），其资源在客户VM之间共享。VMM决定了如何将虚拟资源映射到物理资源：物理资源可以分时共享、划分，甚至可以在软件内模拟。VMM比传统操作系统小得多，VMM的一个隔离部分大约只有10000行代码。

一般来说，处理器虚拟化的成本取决于工作负载。用户级别的处理器操作密集型程序（比如SPECCPU2006）的虚拟化开销为零，这是因为很少会调用操作系统，所有程序都以原速运行。与之相对，I/O密集型工作负载通常也是操作系统密集型的，会执行许多系统调用（以满足I/O需求）和特权指令（频繁使用会导致高昂的虚拟化开销）。这一开销的大小取决于必须由VMM模拟的指令数和模拟这些指令的缓慢程度。因此，如果客户VM与宿主机运行的ISA相同，则这个体系结构和VMM的目标就是直接在原始硬件上运行几乎所有指令，这与我们的预期一致。如果工作负载也是I/O密集型的，那么由于处理器经常要等待I/O，所以处理器虚拟化的成本可以完全被较低的处理器利用率所掩盖。

尽管我们这里关心的是VM提供保护的功能，但VM还有两个具有重大商业价值的优点。

1. 软件管理——VM提供了一种抽象，可以运行整个软件栈，甚至包括诸如DOS之类的旧操作系统。一种典型的部署是用一部分VM运行遗留操作系统，大量VM运行当前稳定的操作系统版本，而一少部分VM用于测试下一个操作系统版本。

2. 硬件管理——需要多台服务器的原因之一是，希望每个应用程序都能在独立的计算机上与其兼容的操作系统一起运行，这种隔离可以提高系统的可靠性。VM使得这些分享软件栈能够独立运行，却共享硬件，从而减少了服务器的数量。另一个例子是，大多数较新的VMM支持将正在运行的VM迁移到另一台计算机上，以均衡负载或从发生故障的硬件中退出。云计算的兴起使得将整个VM迁移到另一个物理处理器的能力变得越来越有用。

以上就是云服务器（比如Amazon的云服务器）依赖虚拟机的两个原因。


==== 对虚拟机监视器的要求

VM监视器必须完成哪些任务?它向客户软件提供一个软件接口，必须使不同客户软件的状态相互隔离，还必须保护自己以免受客户软件的破坏（包括客户操作系统）。定性需求包括：

- 客户软件在VM上的运行情况应当与在原始硬件上完全相同，当然，与性能相关的行为或者因为多个VM共享固定资源所造成的局限除外；

- 客户软件应当不能直接修改实际系统资源的分配。

为了实现处理器的“虚拟化”，VMM必须控制几乎所有操作———对特权状态的访问、地址变换、I/O、异常和中断，即使当前运行的客户VM和操作系统只是临时使用它们也是如此。

例如，在计时器中断时，VMM将挂起当前正在运行的客户VM，保存其状态，处理中断判断接下来运行哪个客户VM，然后载入其状态。依赖计时器中断的客户VM由VMM提供个虚拟计时器和一个仿真计时器中断。

为了进行管理，VMM的管理权限必须高于客户VM，后者通常以用户模式运行；这样还确保任何特权指令的执行都由VMM处理。系统虚拟机的基本需求几乎与上述分页虚拟存储器的需求相同。

- 至少两种处理器模式：系统模式和用户模式。

- 仅在系统模式下可用的指令的一些特权子集，如果在用户模式下执行将会导致异常。所有系统资源都只能通过这些指令进行控制。

==== 虚拟机的指令集体系结构支持

如果在设计ISA期间就为VM做了规划，就很容易减少VMM必须执行的指令数，缩短模拟这些指令所需的时间。如果一种体系结构允许VM直接在硬件上运行，则可以贴上可虚拟化的标签，IBM370体系结构便拥有这个标签。

遗憾的是，由于直到最近才开始考虑将VM用于桌面系统和基于PC的服务器应用程序，所以大多数指令集在设计时没有考虑虚拟化问题，其中包括80x86和大多数原始的RISC体系结构，尽管后者的问题比前者少。x86体系结构的最新特性试图弥补早期的缺陷，而RISC-V明确地包含了对虚拟化的支持。

由于VMM必须确保客户系统只能与虚拟资源进行交互，所以传统的客户操作系统是作为一种用户模式程序在VMM上运行的。因此，如果一个客户操作系统试图通过特权指令访问或修改与硬件资源相关的信息（比如，读取或写人页表指针），它会从用户模式陷人VMM。VMM随后可以对相应的实际资源进行适当的修改。

因此，如果任何以用户模式执行的指令试图读写此类敏感信息就会发生异常，从用户模式陷入VMM特权级别，VMM可以截获它，并根据客户操作系统的需要，向其提供敏感信息的一个虚拟化版本。

如果缺乏此类支持，则必须采取其他措施。VMM必须采取特殊的防范措施。找出所有存在问题的指令，并确保客户操作系统执行它们时能够正常运行，这样自然就会增加VMM的复杂度，降低VM的运行性能。一个有吸引力的扩展允许VM和操作系统在不同的特权级别上操作，每个特权级别都不同于用户级别。通过引入一个额外的特权级别，一些操作系统操作——例如超过了授予用户程序的权限，但不需要VMM的干预（因为它们不会影响任何其他VM）——就可以直接执行，而无须捕获和调用VMM的开销。

==== 虚拟机对虚拟存储器和I/O的影响

由于每个VM中的每个客户操作系统都管理其自己的页表集，所以虚拟存储器的虚拟化就成为另一项挑战。为了实现虚拟存储器的虚拟化，VMM区分了实际存储器（real memory）和物理存储器（physical memory）的概念（这两个词经常被视为同义词），使实际存储器成为虚拟存储器与物理存储器之间的独立、中间级存储器。（有人用虚拟存储器、物理存储器和机器存储器来命名这3个层级。）客户操作系统通过它的页表将虚拟存储器映射到实际存储器，VMM页表将客户的实际存储器映射到物理存储器。虚拟存储器体系结构可以通过页表指定（如在IBMVM/370和80x86中），也可以通过TLB结构指定（如在许多RISC体系结构中）。

VMM没有再为所有存储器访问进行多一层的中间访问，而是维护了一个影子页表（shadowpagetable），直接从客户虚拟地址空间映射到硬件的物理地址空间。通过检测客户页表的所有修改，VMM能保证硬件在变换地址时使用的影子页表项与客户操作系统环境的页表项一一对应，除了用正确的物理页替换了客户表中的实际页。因此，只要客户试图修改它的页表，或者试图访问页表指针，VMM就必须加以捕获。这通常通过以下方法实现：对客户页表提供写保护，并捕获客户操作系统对页表指针的所有访问尝试。前面曾经指出，如果对页表指针的访问属于特权操作，就会很自然地进行捕获。

IBM370体系结构在20世纪70年代添加了一个由VMM管理的中问层，解决了页表问题。客户操作系统和以前一样保存自己的页表，所以就不再需要影子页表了。AMD在其80x86处理器上采用了类似的方案。

在许多RISC计算机中，为了实现TLB的虚拟化，VMM管理实际TLB，并拥有每个客户VM的TLB内容的副本。为了实现这一功能，所有访问TLB的指令都必须被捕获。具有进程ID标签的TLB可以将来自不同VM与VMM的条目混合在一起，所以不需要在切换VM时刷新TLB。与此同时，VMM在后台支持VM的虚拟进程ID与实际进程ID之间的映射。

体系结构中最后一个要虚拟化的部分是I/O。到目前为止，这是系统虚拟化中最困难的一部分，原因在于连接到计算机的I/O设备数在增加，而且这些I/O设备的类型也更加多样。另外一个难题是在多个VM之间共享实际设备，还有一个难题是需要支持不同的设备驱动程序，这一点在同一VM系统上支持不同客户操作系统时尤为困难。为了维持这种VM抽象，可以为每个VM提供每种I/O设备驱动程序的一个通用版本，然后交由VMM来处理实际的I/O。

将虚拟I/O设备映射到物理I/O设备的方法取决于设备类型。例如，VMM通常会对物理磁盘进行分区，为客户VM创建虚拟磁盘，而VMM会维护虚拟磁道与扇区到物理磁道与扇区的映射。网络接口通常会在非常短的时间片内在VM之间共享，VMM的任务就是跟踪虚拟网络地址的消息，以确保客户VM只收到发给自己的消息。

==== 扩展指令集，以实现高效虚拟化和更高的安全性

在过去5到10年里，处理器设计师们，包括AMD和Intel的设计师(还包括一定范围内的ARM设计师），已经引入了指令集扩展来更高效地支持虚拟化。性能提升的两个主要领坡是对页表和TLB（虚拟存储器的基石）的处理，以及I/O，特别是处理中断和DMA。虚拟存储器性能的提升主要是通过避免不必要的TLB刷新和使用嵌套页表机制（IBM几十年前就已采用），而不是一套完整的影子列表。为提升I/O性能，增加了一些体系结构扩展。以允许设备直接使用DMA移动数据（不再需要VMM生成一个副本），而且允许客户操作系统直接处理设备中断和命令。在那些需要进行大量内存管理操作或大量使用I/O的应用中，这些扩展都展现了非常明显的性能提升。

由于现在广泛采用公共云系统来运行一些至关重要的应用程序，所以人们开始关注这些应用程序中的数据安全性。任何一段恶意代码，只要它的访问权限高于必须保证安全的数据，就会危及系统。比如，你正在运行一个信用卡处理应用程序，你就必须确保恶意用户不能获得信用卡号码，即使他们正在使用同一硬件并且有意攻击操作系统甚至VMM。利用虚拟化技术，可以禁止外部用户访问不同VM中的数据，这种方式所提供的保护显然要远高于一个多程序环境。但如果攻击者攻破了VMM，或者可以通过观察另一VM而得到信息，那么上述保护可能还不够。例如，假设攻击者穿透了VMM，他就可以重新映射存储器来访问数据中的任意部分。

或者，也可以在能够访问信用卡的代码中植入特洛伊木马来发起攻击。因为特洛伊木马与信用卡处理应用程序运行在同一个VM中，所以特洛伊木马只需要利用操作系统的缺陷来获取关键数据。大多数网络攻击用到了某种形式的特洛伊木马，通常是利用某种操作系统缺陷，这些木马或是将访问权限返给攻击者，并保持CPU仍然在特权模式下运行，或是允许攻击者上传一段代码，并将它们伪装成操作系统的一部分加以运行。无论是哪种情况，攻击者都获得了对CPU的控制，而且利用更高权限模式，可以进一步获取VM的任意内容。注意，加密本身并不能阻止这种攻击者。如果存储器中的数据没有加密（通常是这种情况），攻击者就可以访问所有此类数据。另外，如果攻击者知道加密密钥的存放位置，就可以自由访问密钥，然后访问加密数据。

Intel推出了一组指令集扩展，称为软件保护扩展（SGX)，允许用户程序创建飞地(enclave)。飞地就是一部分代码和数据，只有在使用时才会进行加密和解密，而且只能使用用户代码提供的密钥。由于飞地总是加密的，所以，针对虚拟存储器或l/O的标准OS操作可以访问这些飞地（例如，移动一个页），但不能提取任何信息。要使一个飞地正常工作，所有用到的代码和数据都必须是这个飞地的组成部分。尽管人们对细粒度保护技术已经讨论了几十年，但一直没有得到多少推动，一方面是由于其开销很高，另一方面是因为其他一些效率更高、侵人性更低的解决方案已经为人们所接受。随着网络攻击和在线机密信息数量的增加，人们开始重新检视用于提升细粒度安全性的技术。与Intel的SGX类似，IBM和AMD最近的处理器都支持对存储器的实时加密。

<<<