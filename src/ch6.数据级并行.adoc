== 数据级并行

数据级并行是指通过同时执行多个数据元素的相同操作来加速计算的并行性。在数据级并行中，处理器能够同时处理多个数据点，通常通过特殊的硬件支持来实现。这种并行方式不依赖于程序中控制结构的分支，而是通过对相同操作的多个数据元素进行并行处理来提高计算速度。数据级并行的核心思想是将大规模数据操作（如数组或矩阵中的每个元素）划分为多个并行任务，利用多核或SIMD（单指令多数据）架构同时执行这些任务，从而显著提高程序的执行效率。

数据级并行广泛应用于科学计算、图像处理、数字信号处理（DSP）、矩阵运算等领域，因为这些应用常常涉及到大量相同的计算操作。通过并行化这些数据操作，可以在很大程度上提高计算性能。实现数据级并行的常见方法包括向量处理器、图形处理单元（GPU）等硬件，通过多条数据通路和同时执行多个计算来加速处理过程。数据级并行不仅提高了处理效率，还使得大规模计算任务能够在较短的时间内完成。

尽管可以在MIMD计算机上编写运行在不同处理器上的独立程序，但是，为了实现更宏大、更协调的目标，程序员通常会编写一个运行在MIMD计算机中所有处理器上的程序，不同的处理器通过条件语句执行不同的代码段。这种编程风格称为单程序多数据流（Single Program Multiple Data,SPMD），它是在MIMD计算机上编程的一种常用方法。最接近多指令流单数据流（MISD）的处理器应该是“流处理器”了，这种处理器在单数据流上以流水线方式执行一系列计算：解析来自网络的输入，分析数据，解压缩，查找匹配，等等。相比之下，与MISD相反的类型——SIMD更受欢迎一些。SIMD(Singlelnstruction stream,Multiple Data streams)计算机对数据向量进行操作。例如，单个SIMD指令将64个数据流发送到64个ALU上，以在单个时钟周期内完成64次加法来将64个数字相加。SIMD的优点是所有的并行执行单元都是同步的，它们都响应自同一程序计数器（PC）中发出的同一指令。从程序员的角度看，这与他们已经很熟悉的SISD的概念非常接近。尽管每个单元将执行相同的指令，但是每个执行单元都有自己的地址寄存器，因此每个单元可以有不同的数据地址。一个顺序应用程序编译后，既可能在组织为SISD的串行硬件上运行，也可能在组织为SIMD的并行硬件上运行。SIMD的初衷是在数个执行单元上分摊控制单元的成本。因此，SIMD的另一个优点是减少了指令带宽和空间—SIMD只需要同时执行代码的一个副本，消息传递的MIMD可能需要在每个处理器中都存有一个副本，而共享存储器的MIMD可能需要多个指令缓存。SIMD在处理for循环中的数组时效果最好。因此，为了在SIMD上并行运行，程序中必须存在大量相同结构的数据，这称为数据级并行(data-level parallelism)。SIMD在处理case和switch语句时效果最差，在这些语句中，每个执行单元必须根据单元内存放的不同数据对这些数据执行不同的操作。存放有错误数据的执行单元必须被禁止执行、以便存放有正确数据的执行单元可以继续工作。在有n个case语句的情况下，SIMD处理器基本上只能以峰值性能的l/n工作。

单指令流多数据流（SIMD）使得一条向量指令代表了多条指令，同时流水化处理多条数据，从而减少了指令获取和解码的带宽。同时由于每条向量指令的行为已知，可以有效避免竞争冒险的出现。

=== 向量体系结构

对SIMD更加古老、更加优雅的解释被称为向量体系结构，这与Seymour Cray在20世纪70年代开始设计的计算机密切相关。这种体系结构与具有大量数据级并行的问题非常匹配。与早期的阵列处理器一样，64个ALU并不是同时执行64次加法，而是采用向量体系结构流水化ALU，从而以更低的成本获得良好的性能。向量体系结构的基本原理是从内存中收集数据元，将它们按顺序放人一大组寄存器中，使用流水化的执行单元在寄存器中依次对它们进行操作，然后将结果写回内存。向量体系结构的一个关键特性是拥有一组向量寄存器。这样，一个向量体系结构中可能具有32个向量寄存器，每个寄存器包含64个64位宽的数据元。

向量处理器大大降低了动态指令带宽。相比之下向量RISC-V相对于传统的RISC-V体系结构大大减少了指令数量。这种减少是因为一次向量操作可以处理多个个数据元，并且在RISC-V指令体系结构中近一半的循环开销指令在向量代码中无须存在。指令取指和执行次数的减少的确可以节省能耗。

另一个重要的区别是流水线冒险的频率不同。在传统的RISC-V代码中，如每个fadd.d指令必须等待fmul.d指令完成，每个fsd指令必须等待fadd.d指令完成，而且每个fadd.d指令和fmul.d指令还必须等待fld指令完成。在向量处理器中，每个向量指令只会停顿每个向量中的第一个数据元，然后后续数据元将顺利地沿着流水线流动。因此，每个向量操作仅需要一次流水线停顿，而不是每个数据元都停顿一次。

通过循环展开可以消除传统RISC-V上的流水线停顿。但是，指令带宽的巨大差异依然不能减少。因为向量中的各数据元是相互独立的，所以它们可以并行操作，这非常类似于Intel x86AVX指令中的子字并行。所有现代向量计算机都具有向量功能单元，该单元具有多个并行流水线，每个时钟周期可计算出两个或更多个结果。

**向量与标量**

与传统指令系统体系结构（在本节中被称为标量体系结构）相比，向量指令具有几个重要的属性：

- 单个向量指令指定了大量工作——相当于执行了完整的循环。正因为这样，指令取指和译码带宽大大减少。

- 通过使用向量指令，编译器或程序员确认了向量中的每个结果都是独立的，因此硬件无须再检查向量指令内的数据冒险。

- 当程序中存在数据级并行时，相比使用MIMD多处理器，使用向量体系结构和编译器的组合更容易写出高效的应用程序。

- 硬件只需要在两条向量指令之间检查向量操作数之间的数据冒险，而无须检查向量中的每个数据元。减少检查的次数可以节省能耗和时间。

- 访问存储器的向量指令具有确定的访问模式。如果向量中的数据元位置都是连续的，则可以从一组存储器中交叉访问数据块，从而快速获取向量。因此，对整个向量而言，主存储器的延迟开销看上去只有一次，而不是对向量中的每个字都产生一次。

- 因为整个循环被具有已知行为的向量指令所取代，所以通常由循环引发的控制冒险不再存在。

- 与标量体系结构相比，节省的指令带宽和冒险检查以及对存储器带宽的有效利用，使得向量体系结构在功耗和能耗方面更具有优势。

综上所述，在同等数据量的前提下，向量操作可以比一组标量操作序列更快完成。如果应用程序当中经常使用这些向量操作，设计者将更有动力在设计中加入向量单元。

==== 向量执行时间

向量运算序列的执行时间主要取决于3个因素：

1. 操作数向量的长度；

2. 操作之间的结构冒险；

3. 多个向量操作间的数据依赖关系。

给定向量长度和启动速率（向量单元接受新操作数或生成新结果的速率），可以计算出一条向量指令的执行时间。

所有现代向量计算机都有具备多条并行流水线（或通道，lane）的向量功能单元。这些道在每个时钟周期可以生成一个或更多结果。与此同时这些计算机还可能拥有一些未完全流水化的功能单元。为简便起见，我们的RV64V实现方式有一条通道，各个操作的启动速率为每个时钟周期一个元素。因此，一条向量指令的执行时间（以时钟周期为单位）大约就是向量长度。

为了简化关于向量执行和向量性能的讨论，我们使用护航指令组（convoy）的概念，它是一组可以一起执行的向量指令。护航指令组中的指令不能包含任何结构冒险，否则需要在不同护航指令组中先后启动这些指令。可以通过计算护航指令组的数目来估计一段代码的性能。为了保持分析过程简单，假定在开始执行任意其他指令（标量或向量）之前，护航指令必须已经执行完成。

除了具有结构冒险的向量指令序列之外，具有写后读相关冒险的序列也应该位于单独的护航指令组中。然而，链接（chaining）操作可以让它们位于同一护航指令组中，因为链接操作允许向量操作在其向量源操作数的各个元素可用时立即启动：链中第一个功能单元的结果被“前递”给第二个功能单元。实践中经常采用以下方式来实现链接：允许处理器同时读写一个特定的向量寄存器，不过读写的是不同元素。早期的链接实现类似于标量流水线中的前递，但这限制了链中源指令与目标指令的时序。最近的链接实现采用灵活链接，这种方式允许向量指令链接到几乎任意其他活动的向量指令，只要不生成结构冒险就行。所有现代向量体系结构都支持灵活链接，这也是本章的假设之一。

为了将护航指令组转换为执行时间，需要一种度量来估计护航指令组的长度。这种度量被称为钟鸣(chime)，就是执行一个护航指令组所需的时间单位。因此，执行由m个护航指令组构成的向量序列需要m个钟鸣。当向量长度为n时，对于简单的RV64V实现来说，大约为stem:[m \times n]个时钟周期。

钟鸣近似值忽略了处理器特有的一些额外开销，其中许多开销依赖于向量长度。因此，以钟鸣为单位测量时间时，对于长向量的近似要优于对短向量的近似。我们将使用钟鸣测量结果（而不是每个结果的时钟周期）来明确表示我们忽略了某些开销。

如果知道向量序列中的护航指令组数，就知道了用钟鸣表示的执行时间。在以钟鸣为单位测量执行时间时，所忽略的一个额外时间开销是对单个时钟周期内启动多条向量指令的限制。如果在一个时钟周期内只能发射一条向量指令（大多数向量处理器均如此），那钟鸣数会低估护航指令组的实际执行时间。由于向量的长度通常远大于护航指令组中的指令数，所以我们简单地假定这个护航指令组是在一个钟鸣中执行的。

另一个额外时间开销要比指令发射数量的限制重要得多。钟鸣模型忽略的最重要的开销是向量启动延迟（start-up time），即向量功能单元的流水线被向量指令填满之前，以时钟周期为单位的延迟。启动延迟主要由向量功能单元的流水线延迟决定。对于RV64V.我们使用与Cray-l相同的流水线深度，不过在更现代的处理器中，这些延迟有增加的趋势。特别是向量载入操作的延迟。所有功能单元都被完全流水化。浮点加的流水线深度为6个时钟周期，浮点乘为7个时钟周期，浮点除为20个时钟周期，向量载入为12个时钟周期。

有了这些向量基础知识之后，接下来的几节将介绍一些优化方式，这些优化可以提高性能，或者增加在向量体系结构中良好运行的程序类型。具体来说，它们将回答如下问题。

- 向量处理器怎样执行单个向量才能在每个时钟周期执行多于一个元素？每个时钟周期处理多个元素可以提高性能。

- 向量处理器如何处理那些向量长度与最大向量长度不匹配的程序？由于大多数应用程序向量与硬件体系结构向量长度不匹配，所以需要一种高效的解决方案来处理这一常见情景。

- 如果要向量化的代码中含有IF语句，会发生什么?如果可以高效地处理条件语句，就能向量化更多的代码。

- 向量处理器需要从存储器系统中获得什么？如果没有足够的存储带宽，向量执行可能会徒劳无益。

- 向量处理器如何处理多维矩阵？为使向量体系结构能够很好地工作，必须对这个常见数据结构进行向量化。

- 向量处理器如何处理稀疏矩阵？这一常见数据结构也必须进行向量化。

- 如何为向量计算机编程？如果体系结构方面的创新不能与编程语言及其编译器技术相匹配，就不可能得到广泛应用。4.2.4多条通道：每个时钟周期处理多个元素向量指令集的一个关键好处是，软件仅使用一条很短的指令就能向硬件传送大量并行任务。一条向量指令可以包含数十个独立运算，而其编码使用的位数与一条传统的标量指令相同。为了执行一条向量指令，向量指令的并行语义允许实现使用一个深度流水化的功能单元（就像我们研究过的RV64V实现一样）、一组并行功能单元，或者并行功能单元与流水线功能单元的组合。图4-2说明了如何使用并行流水线来执行一个向量加法指令，从而提高向量性能。

==== 单指令流多数据流（SIMD）

单指令流多数据流（SIMD, Single Instruction, Multiple Data）是一种数据级并行的计算模型，其中多个数据元素同时执行相同的指令。SIMD 架构允许处理器在一个时钟周期内对多个数据进行并行操作，而不需要多条独立的指令流。简而言之，SIMD 通过将同一指令应用于多个数据元素来加速计算过程。

在 SIMD 中，处理器的每个指令执行都针对多个数据元素，这些数据元素通常是数组、矩阵或向量中的元素。与传统的顺序执行单个数据流的处理方式不同，SIMD 允许通过并行计算在单个指令周期内完成多个操作。SIMD 利用特殊的硬件结构，如向量寄存器和多路复用器，来处理多个数据元素，这使得它能够在图形处理、音频处理、视频解码和科学计算等领域大幅提升性能。

SIMD 的特点之一是，它不需要对控制流进行并行化，而只关注数据并行化。这使得 SIMD 架构非常适合于那些数据操作模式高度重复且相似的任务，例如图像处理中的像素操作或向量运算。

SIMD 的优点包括显著提高运算速度和效率，尤其在需要大量相同计算的任务中，如大规模矩阵运算、图形渲染和数字信号处理。此外，SIMD 在硬件实现上较为简单，能够有效提高处理器的吞吐量。

然而，SIMD 也有其局限性。它依赖于特定的程序结构和数据格式，只有在数据可以并行处理时才能发挥优势。如果程序中数据间依赖较多或存在复杂的控制流结构，SIMD 的优势就会受到限制。此外，SIMD 需要硬件的支持，现代的多核处理器和图形处理单元（GPU）普遍支持 SIMD 指令集，如 Intel 的 SSE 和 AVX，AMD 的 3DNow!，以及 ARM 的 NEON 等。

向量指令集的一个关键好处是，软件仅使用一条很短的指令就能向硬件传送大量并行任务。一条向量指令可以包含数十个独立运算，而其编码使用的位数与一条传统的标量指令相同。为了执行一条向量指令，向量指令的并行语义允许实现使用一个深度流水化的功能单元（就像RV64V实现一样）、一组并行功能单元，或者并行功能单元与流水线功能单元的组合。

RV64V指令集有一个特性：所有向量算术指令只允许一个向量寄存器的第N个元素与其他向量寄存器的第N个元素进行运算。这一特性极大地简化了高度并行向量单元的设计，该单元可以构造为多个并行通道。和高速公路一样，我们可以通过添加更多通道来提高向量单元的峰值吞吐量。如从单通道变为四通道之后，一次钟鸣的时钟周期数由32个降为8个。若想让多通道带来优势，应用程序和体系结构都必须支持长向量；否则，它们会快速执行，耗尽指令带宽，并需要ILP技术提供足够的向量指令。

每条通道都包含向量寄存器堆的一部分以及来自每个向量功能单元的一条执行流水线。每个向量功能单元使用多条流水线（每条通道一条流水线），以每个时钟周期一个元素组的速度执行向量指令。第一条通道保存所有向量寄存器的第一个元素（元素0），所以任何向量指令的第一个元素的源操作数与目标操作数都在第一通道中。这种分配方式使得该通道本地的算术流水线无须与其他通道通信就能完成运算。通过避免通道间的通信，减少了构建高并行执行单元所需要的连接成本与寄存器堆端口，同时也解释了向量计算机为什么能够在每个时钟周期内完成多达64个运算（16通道，每条通道包含2个算术单元和2个载人存储单元）。

增加多条通道是提高向量性能的一种常用技术，它几乎不需要增加控制复杂性，也不需要对现有机器代码进行修改。它还允许设计人员在晶片面积、时钟频率、电压和能耗之间进行权衡啊，而且不需要牺牲峰值性能。如果向量处理器的时钟频率减半，那么将通道数量加倍就能保证原峰值性能。

==== 向量长度寄存器：处理未知向量长度的循环

条带挖掘技术使得每个向量运算都是针对向量大小小于或等于最大向量长度的情况来完成的。

向量寄存器处理器有一个自然向量长度，这一长度由最大向量长度（mv1）决定。该长度不大可能与程序中的实际向量长度相匹配。此外，在实际程序中，待定向量运算的长度在编译时通常是未知的。事实上，一段代码可能需要不同的向量长度。例如，考虑以下代码：

[source,c]
----
for (i = 0; i < n; i=i+1)
	Y[i] = a * X[i] + Y[i];
----

所有这些向量运算的大小都取决于n，但n的取值甚至可能直到运行时才知道。n的值还可能是某个函数（该函数中包含上述循环）的参数，因而会在执行时发生变化。

这些问题的解决方案是添加一个向量长度寄存器（vl）。vl控制所有向量运算的长度，包括向量载入与存储运算。但vl中的值不能大于最大向量长度（mvl）。只要实际长度小于或等于ml，就能解决上述问题。这个参数意味着向量寄存器的长度可以随着计算机的发展而增大，而不需要改变指令集。

如果n的值在编译时未知，因而可能大于mv1，该怎么办呢?为了解决第二个问题（向量长于最大长度），可以使用一种名为条带挖掘（stripmining）的技术。条带挖掘是指生成一些代码，使每个向量运算都是针对向量大小小于或等于mv1的情况来完成的。一个循环处理迭代数为mv1倍数的情况，另一个循环处理剩下的迭代，这些迭代数量必须小于mvl。RISC-V有一种更好的方法，不用为条带挖掘单独使用一个循环。指令setvl取mvl和循环变量n中的较小的那个值写入vl（及另一个临时标量寄存器中）。如果循环的迭代次数大于n，则该循环最快能够计算mvl个值，所以setvl将vl设定为mvl。如果n小于mvl.在循环的最后一次迭代中，它应当仅计算最后n个元素，所以setvl将vl设定为n。setvl还会写入另一个标量寄存器，用于帮助之后的循环进行记录。

==== 谓词寄存器（Predicate Registers）：处理向量循环中的IF语句

**允许处理器在执行指令时跳过某些操作，从而实现分支控制**

根据Amdahl定律我们知道，中低度向量化程序的加速比非常有限。循环内部存在条件（IF语句）与使用稀疏矩阵是向量化程度较低的两个主要原因。循环中包含IF语句的程序无法使用前面讨论的技术以向量模式运行，因为IF语句会在循环中引入控制相关。同样，我们无法利用前面看到的各项功能高效地实现稀疏矩阵。下面讨论处理条件执行的策略，稀疏矩阵留待后文讨论。

考虑以C语言编写的以下循环：

[source,c]
----
for (i = 0; i < 64; i=i+1)
	if (X[i] != 0)
		X[i] = X[i] - Y[i];
----

由于这一循环体需要条件执行，所以它通常不能向量化；但是，如果我们可以选择性地只执行`X[i] != 0`的循环体，那就可以实现减法的向量化。

实现这一功能的常见扩展称为向量掩码控制（vector-mask control)。在RV64V中，谓词寄存器保存此掩码，为一条向量指令中的每个元素运算提供了条件执行方式。这些寄存器使用一个布尔向量来控制向量指令的执行，就像条件执行指令使用布尔条件来判断是否要执行一个标量指令一样。当谓词寄存器p0被置位时，所有后续向量指令都仅针对一部分向量元素执行，这些元素在谓词寄存器中的对应项为1。如果目标向量寄存器中的某些项在谓词寄存器中的对应值为0，那它们就不会受到向量运算的影响。和向量寄存器一样，谓词寄存器也是可配置、可禁用的。启用一个谓词寄存器会将它初始化为全1，也就是说，后续的向量指令运算将对所有向量元素运行。

使用向量掩码寄存器确实是有开销的。对于标量体系结构，在条件不满足时，条件执行的指令仍然需要执行时间。无论如何，通过消除分支和相关的控制依赖确实可以加快条件指令的执行速度.即使这有时会做一些无用功。与此类似，采用向量掩码执行的向量指令仍然需要相同的执行时间，即使掩码为0的元素也是如此。同样，即使掩码中有大量0，使用向量掩码控制的速度仍然远快于使用标量模式的速度。向量处理器与GPU之间的一个区别是处理条件语句的方式。向量处理器将谓词寄存器作为体系结构状态的一部分，并且依靠编译器来显式地操控掩码寄存器。而GPU使用硬件来操控GPU软件无法看到的内部掩码寄存器，以实现相同效果。在这两种情况下，无论相应的掩码位是1还是0，硬件都要花时间执行向量元素，所以GFLOPS速率在使别掩码时会下降。

==== 存储体

载入存储向量单元的行为要比算术功能单元复杂得多。载入操作的启动延迟就是它从存储器向寄存器中载入第一个字的时间。如果可以在无停顿的情况下提供向量的其他元素，那么向量启动速率就等于提取或存储新字的速度。与较简单的功能单元不同，这一启动速率不一定是一个时钟周期，因为存储体（bank）的停顿可能会降低实际吞吐量。

一般情况下，载入/存储单元的初始化延迟要高于算术单元——在许多处理器中要多于100个时钟周期。对于RV64V，我们假定初始化延迟为12个时钟周期，与Cray-1相同。（最近的向量计算机使用缓存来降低向量载入与存储的延迟。）

为了保持每个时钟周期提取或存储一个字的启动速率，存储器系统必须能够提供或接受较多的数据。将访问对象分散在多个独立的存储体中，通常可以保证所需速率。稍后你会看到，拥有大量存储体对于处理那些访问多行或多列数据的向量载入或存储指令非常有用。

大多数向量处理器使用存储体，这允许进行多个独立访问，而不是简单的存储器交错，原因如下：

1. 许多向量计算机支持每个时钟周期执行多个载入或存储操作，访问存储体的周期时间通常比处理器周期时间高几倍。为了支持多个载入或存储操作的同时访问，存储器系统需要有多个存储体，还要能够独立控制对这些存储体的寻址。

2. 大多数向量处理器支持载入或存储非连续的数据字。在这种情况下，需要进行独立的组寻址，而不是交叉寻址。

3. 大多数向量计算机支持多个处理器共享同一存储器系统，所以每个处理器会生成其自己的独立寻址流。

从更宏观的角度来看，向量载入/存储单元的角色类似于向量处理器中的预取单元，它们都通过向处理器提供数据流来提供数据带宽。

==== 步幅

向量中的相邻元素在存储器中的位置不一定是连续的。考虑下面这段用C语言编写的非常简单的矩阵乘法代码：

[source,c]
----
for (i = 0;i < 100: i=i+1)
	for (j = 0;j < 100; j=j+1) {
		A[i][j] = 0.0;
		for (k = 0;k < 100; k = k+1)
			A[i][j] = A[i][j] + B[i][j] * D[k][j];
	}
----

我们可以将B的每一行与D的每一列的乘法向量化，以k为索引变量对内层循环进行条带挖掘。

为此，我们必须考虑如何对B中的相邻元素及D中的相邻元素进行寻址。在为数组分配存诸器时，该数组被线性化，并且必须以行主次序（如C语言）或列主次序（如Fortran语言）进行布局。这种线性化意味着行中的元素或者列中的元素在存储器中是不相邻的。例如，上面的C代码以行主次序来分配存储器，所以内层循环中各次迭代在访问D的元素时，这些元素之间的间隔等于行大小乘以8（每一项的字节数），共800字节。在基于缓存的系统中，通过分块有可能提高局部性。对于没有缓存的向量处理器，需要用另一种方法来提取在存储器中不相邻的向量元素。

对于那些要收集到一个寄存器中的元素，它们之间的距离称为步幅（stride）。在这个例子中，矩阵D的步幅为l00个双字(800字节)，矩阵B的步幅为1个双字（8字节）。对于以列为主的排序（Fortran 语言采用这一顺序）.这两个步幅会颠倒过来：矩阵D的步幅将为1.也就是说连续元素之间相隔1个双字（8字节），而矩阵B的步幅为100，也就是100个双字（800字节）。因此，如果不对循环进行重新排序，编译器就不能隐藏矩阵B和0中连续元素之间的长距离。

一旦将向量载入向量寄存器，它的表现就好像它的元素在逻辑上是相邻的。因此，仅利用具有步幅功能的向量载入及向量存储操作，向量处理器就可以处理大于1的步幅，这种步幅称为非单位步幅（nonunit stride）。向量处理器的一大优势就是能够访问非连续存储地址，并将其重组成一个稠密的结构。

缓存在本质上是处理单位步幅数据的。增加块大小有助于降低大型科学数据集（步幅为单位步幅）的缺失率，但增大块大小也可能会对那些以非单位步幅访问的数据产生负面影响。尽管分块技术可以解决其中一些问题但在某些问题上，高效访问非连续数据的能力仍然是向量处理器的一个优势。

在RV64V中，可寻址单位为1字节，所以我们示例中的步幅将为800。由于矩阵的大小在编译时可能是未知的，或者就像向量长度一样，在每次执行相同语句时可能会发生变化，所以必须对步幅值进行动态计算。像向量起始地址一样，向量步幅可以放在通用寄存器中。然后，RV64V指令VLDS（load vector with stride）将向量提取到向量寄存器中。同样，在存储非单位步幅向量时，使用指令VSTS(store vector with stride)。

支持大于1的步幅会使存储器系统变得复杂。一旦引人非单位步幅，就可能频繁访问同个存储体。当多个访问争用一个存储体时，就会发生存储体冲突，从而使某个访问陷入停领。如果满足以下条件，就会产生存储体冲突，进而造成停顿：

[stem]
++++
\frac{bank数}{步幅与bank数的最大公约数} < bank繁忙时间
++++

==== 向量体系中稀疏矩阵的处理

前面曾经提到，稀疏矩阵很常见，所以使用一些技术来让使用稀疏矩阵的程序在向量模式下执行是很重要的。在稀疏矩阵中，向量元素通常是以某种压缩形式存储的，然后被间接访问。假定有一种简化的稀疏结构，我们可能会看到类似下面的代码：

[source,c]
----
for (i = 0; i < n; i=i+1)
	A[K[i]] = A[K[i]] + C[M[i]];
----

这段代码实现数组A与数组C的稀疏向量求和，用索引向量K和M来指定A与C中的非零元素。（A和C的非零元素数必须相等，为n，所以K和M大小相同。）

支持稀疏矩阵的主要机制是采用索引向量的集中一分散(gather-scatter)操作。这种操作的目的是支持在稀疏矩阵的压缩表示（即不包含零）和正常表示（即包含零）之间进行转换。集中操作取得索引向量(index vector)，并在此向量中提取元素，元素位置等于基础地址加上索引向量中给定的偏移量。其结果是向量寄存器中的一个密集向量。在以密集形式对这些元素进行操作之后，可以再使用同一索引向量，通过分散存储操作，以扩展方式存储该稀疏向量。对此类操作的硬件支持称为集中一分散，几乎所有现代向量处理器都具备这一功能。RV64V指令为vldx（载入索引向量，也就是集中）和vstx（存储索引向量，也就是分散）。例如，如果x5、x6、x7和x28中包含以上序列中向量的起始地址，就可以用向量指令来对内层循环进行编码，如下所示：

[source,asm]
----
vsetdcfg 4*FP64 		#4个64位浮点向量寄存器
vld		 v0. x7 		#载入K[]
vldx	 v1. x5. v0) 	#载入A[K[]]
vld		 v2. x28 		#载入M[]
vldi 	 v3. x6. v2)	#载入C[M[]]
vadd	 vl. vl. v3
vstx	 v1. x5. v0)	#存储 A[K[]]
vdisable				#禁用向量寄存器
----

利用这一技术，可以以向量模式运行访问稀疏矩阵的代码。简单的向量化编译器无法自动将以上源代码向量化，因为编译器不知道K的元素是不同的值，因此也就不存在相关性。所以。需要程序员通过显式地在代码中指示编译器，可以放心地以向量模式运行这个循环。

尽管索引载入与存储（集中与分散）操作都可以流水化，但由于存储体在开始执行指令时是未知的，所以它们的运行速度通常远低于非索引载入或存储操作。寄存器堆还必须在向量单元的通道之间提供通信，以支持集中和分散操作。

执行集中和分散操作的每个元素都有各自的地址，所以不能对它们进行分体处理，而且在存储器系统的许多位置都可能存在冲突。因此，即使在基于缓存的系统上，每次访问也会造成严重的延迟。但是，如果架构师不是对这种不可预测的访问采取放任态度，而是针对这一情景进行设计，使用更多的硬件资源，那么存储器系统就能提供更好的性能。

在GPU中，所有载入都是集中操作，所有存储都是分散操作，因为没有单独的指令限制地址必须是连续的。为了将可能较慢的集中和分散操作转换为更高效的存储器单位步幅访问，GPU硬件必须在执行期间识别顺序地址，并且GPU程序员必须确保一次集中或分散操作中的所有地址都位于相邻位置。

==== 向量体系结构编程

向量体系结构的优势在于，编译器可以在编译时告诉程序员某段代码是否会向量化，通常还会给出一些提示，说明这段代码为什么没有向量化。这种简单的执行模型可以让其他领域的专家快速掌握修改代码来提高性能的方法，并提示编译器特定操作（比如集中一分散式的访存请求）间不存在依赖关系以提高性能。这就是编译器与程序员之间的对话，每一方都就如何提高性能给对方一些提示，从而简化向量计算机的编程。

今天，影响程序在向量模式下能否成功运行的主要因素是程序本身的结构：循环是否有真正的数据相关？能否调整它们的结构，使其没有此类相关？这一因素受算法选择的影响，在一定程度上还受编码方式的影响。

=== 图形处理器

GPU（Graphics Processing Unit，图形处理单元）是专门为图形和视频处理设计的高性能处理器。最初，GPU 主要用于加速计算机图形渲染，如二维和三维图形的生成、图像处理和显示。随着技术的发展，GPU 的计算能力逐渐超越了传统图形处理，成为一种强大的通用计算平台，尤其在需要并行处理的大规模数据计算中，具有显著的优势。

GPU 的核心特点是拥有大量的计算核心，能够同时处理大量数据。与 CPU（中央处理单元）不同，CPU 主要优化了顺序计算和较复杂的控制逻辑，而 GPU 更加专注于大规模并行计算，特别是数据并行任务。GPU 中的计算核心通常是简化的处理单元，专门设计用于处理并行任务，因此它能够高效地执行大量相同类型的计算操作，例如矩阵乘法、向量运算和像素处理。

除了传统的图形渲染，现代 GPU 还广泛应用于科学计算、人工智能、深度学习、数据分析和物理模拟等领域。GPU 的高吞吐量和并行计算能力使得它非常适合进行大规模的计算任务，特别是在深度学习中的矩阵运算和神经网络训练中，GPU 显示出了远超 CPU 的性能。GPU 加速器通过并行化的计算模式，可以将数据密集型任务的处理时间大大缩短。

GPU 的优点包括高并行度、高吞吐量和对浮点运算的优化。它能够处理大量的计算任务，特别适用于图形渲染、图像处理、科学计算和机器学习等应用。GPU 在能效方面也表现出色，因为它们能够在较低的时钟频率下，通过大量的并行计算来完成工作。下表为CPU与GPU的区别：

[options="header"]
|====
|特点 |CPU |GPU

|设计目标
|专为处理广泛的任务和复杂的控制逻辑而设计，通常用来执行操作系统、应用程序以及各种计算任务。它擅长处理单线程任务和需要高指令集支持的复杂控制流。
|最初为图形渲染任务设计，专注于高效执行大量的并行数据计算。随着计算技术的发展，GPU 被扩展用于高并行计算的通用任务，如科学计算、机器学习和数据处理。

|计算核心
|通常拥有较少的核心（2到32个核心），每个核心的时钟频率较高。每个核心都具备较强的运算能力，适用于执行复杂的顺序计算和多任务处理。
|包含大量的处理核心（数百个甚至更多），每个核心相对简单，能够执行大量的相同操作。GPU 的设计目的是通过并行处理大量数据来加速特定类型的计算，如图像处理和大规模矩阵运算。

|并行性
|擅长处理少量复杂任务，尤其是需要频繁分支、控制流和复杂计算的任务。它的并行性通常体现在多核并行处理上，但核数较少，主要依靠较高的时钟频率来提升性能。
|天生设计为处理大量并行计算任务，具有极高的数据并行性。GPU 通过数百或数千个核心同时处理相同的操作，从而加速处理大规模数据集，尤其是在没有复杂控制流的情况下，如图像、视频和矩阵计算。

|处理能力
|虽然单个核心的处理能力较强，能够有效处理复杂的算法、分支判断和系统管理任务，但它在处理大规模并行任务时并不高效。
|在执行大规模数据并行任务时，比 CPU 更高效。GPU 在执行重复性计算任务（如图形渲染、深度学习训练、科学计算等）时，能够显著提高吞吐量，尤其适合大规模矩阵运算和向量计算。

|存储结构
|内存层次结构复杂，通常有多级缓存（L1、L2、L3），并且具有直接访问较大的主存（RAM）的能力。CPU 的缓存和内存结构优化了频繁的指令和数据访问。
|也有自己的内存结构，通常包括高速的共享内存和全局内存。GPU 使用统一的内存访问模式来处理大规模并行任务，但内存访问的延迟较高，因此通常需要通过优化内存访问模式来提高性能。

|任务适应性
|适合处理复杂的逻辑运算、输入/输出操作、网络管理和多任务操作等，能够执行多种类型的程序和应用，尤其是那些包含大量分支、条件判断和动态任务的程序。
|非常适合数据密集型的任务，如图像渲染、科学计算、大规模并行处理和机器学习训练等。GPU 更适合没有太多分支、顺序依赖的计算，尤其是在需要大规模并行计算的应用中表现突出。

|编程模型
|拥有复杂的指令集和强大的控制逻辑，支持多种编程语言和并发模型。程序员可以通过常见的编程模型（如多线程编程）来编写代码。
|虽然现代 GPU 也支持并行计算任务，但其编程模型相对复杂，需要使用特定的编程框架（如 CUDA 或 OpenCL）来实现并行计算。GPU 的计算模型强调数据并行性，编程时需要考虑数据访问模式和内存管理。

|能效
|由于每个核心都设计得更为复杂，CPU 在处理多线程和高频率任务时能效较高，但在大规模并行任务上的能效较低。
|尽管每个核心较简单，但由于其高度并行的架构，GPU 能在执行大规模并行计算时提供优异的能效。GPU 的并行计算能力在处理密集型计算时比 CPU 高效得多，尤其在图形处理和机器学习等任务中能耗相对较低。

|适用领域
|适用于操作系统、数据库管理、大多数应用程序和多任务处理等广泛领域，特别是需要高计算精度和灵活性的任务。
|主要用于图形渲染、科学计算、深度学习、大数据分析、加密解密等领域，在图像和视频处理、AI 训练和推理等方面有着卓越的表现。

|====

=== 检测与增强循环级并行
我们准确地定义一个循环何时是并行的（即可向量化的）、相关性如何阻碍循环成为并行的，以及用于消除几类相关性的技术。发现和利用循环级并行，对于利用DLP和TLP以及附录H中介绍的更激进的静态ILP方法（例如，VLIW）都至关重要。

循环级并行通常在源代码级别或接近源代码级别进行研究，而对ILP的大多数分析是在编译器生成指令之后进行的。循环级分析需要确定循环的操作数在这个循环的各次迭代之间存在哪种相关性。就目前来说，我们将仅考虑数据相关；在某一时刻写入操作数，并在稍后的时刻读取时，会出现这种相关性。

循环级并行的分析主要是判断后续迭代中的数据访问是否依赖于在先前迭代中生成的数据值；这种相关称为跨迭代相关(loop-carried dependence)。为了了解一个循环是并行的，我们首先看看源代码：

[source,c]
----
for (i = 999; i >= 0; i=i-1)
	x[i] = x[i] + s;
----

在这个循环中，对x[i]的两次使用是相关的，但这是同一个迭代内的相关，不是跨迭代相关。在不同迭代中对i的连续使用之间存在跨迭代相关，但这种相关涉及一个容易识别和消除的归纳变量。

因为要寻找循环之间的并行，需要识别诸如循环、数组引用和归纳变量计算之类的结构，所以与机器码级别相比，编译器在源代码级别或相近源代码级别进行这一分析要更轻松一些。

我们的分析需要首先找出所有跨迭代相关。这一相关信息是不确切的，也就是说，它告诉我们此相关可能存在。考虑以下示例：

[source,c]
----
for (i = 0; i < 100; i=i+1) {
	A[i] = B[i] + C[i];
	D[i] = A[i] + E[i];
}
----

这个例子中对A的第二次引用不需要转换为载入指令，因为我们知道这个值是由上一个语句计算并存储的。因此，对A的第二个引用可能就是引用计算A的寄存器。为了执行这一优化，需要知道这两个引用总是指向同一存储器地址，而且不存在对相同位置的干扰访问。通常，数据相关分析告诉我们只有一个引用可能依赖于另一个引用；要确定两个引用一定指向同一地址.需要进行更复杂的分析。在上面的例子中，进行这一简单分析就足够了，因为这两个引用都处于同一基本块中。

跨迭代相关经常是递推(recurrence）形式。当一个变量基于它在先前迭代中的取值进行定义时，就会发生递推；这个先前迭代往往就是前面的迭代，如以下代码段所示：

[source,c]
----
for (i = 1; i < 100; i=i+1)
	Y[i] = Y[i-1] + Y[i];
----

检测递推非常重要，原因有二：一些体系结构（特别是向量计算机）对执行递推提供了特殊支持；而对于IP而言，递推形式的跨迭代相关有可能并不会成为开发并行性的阻碍。

==== 查找相关

显然，查找程序中的相关对于确定哪些循环可能包含并行以及消除名称相关都很重要。C或C+\+等语言中存在数组和指针，Fortran中存在按引用传递的参数，这些语法都增加了相关分析的复杂度。由于标量变量引用明确指向名称，所以用别名对它们进行分析是比较轻松的，因为指针和引用参数会增加分析过程的复杂性和不确定性。

编译器通常是如何检测相关的呢？几乎所有相关分析算法都假定数组索引是仿射的（afine)。用最简单的话说，如果一维数组索引可以写为stem:[a \times i + b]的形式，其中a和b是常数，i是循环索引变量，那么它就是仿射的。如果多维数组每一维的索引都是仿射的，就称这个多维数组的索引是仿射的。稀疏数组访问（其典型形式为x[y[i]]）是非仿射访问的主要示例之一。

要判断一个循环中对同一数组的两次引用之间是否存在相关，等价于判断两个仿射函数能否针对不同索引取同一个值（这些索引没有超出循环范围）。例如，假定我们以索引值stem:[a \times i + b]存储了一个数组元素，并以索引值stem:[c \times i + d]从同一数组中载入，其中i是FOR循环索引变量，其变化范围是m~n。如果满足以下两个条件，则存在相关性。

1. 有两个迭代索引j和k.它们都在循环范围内，即m≤j≤n.m≤k≤n。

2. 此循环以索引stem:[a \times j + b]存储一个数组元素，然后以stem:[c \times k + d]提取同一数组元素，即stem:[a \times j + b = c \times k + d]。

一般来说，我们在编译时不能判断是否存在相关。例如，a、b、c和d的值可能是未知的（它们可能是其他数组中的值），从而不可能判断是否存在相关。在其他情况下，在编译时进行相关测试的开销可能非常高，但的确可以确定是否存在相关：例如，可能要依靠多重嵌套循环的迭代索引来进行访问。但是，许多条目主要包含一些简单的索引，其中a、b、c和d都是常数。对于这些情况，有可能为相关性设计合理的编译时测试。

举个例子，最大公约数（GCD）测试非常简单，但足以判定不存在相关的情况。它基于以下事实：如果存在跨迭代相关，那么GCD(c,a)必须能够整除(d-b)。（回想一下，有两个整数x、y，在计算y除法运算时，如果能够找到一个整数商，使运算结果没有余数，则说x能够整除y。）

GCD不能整除足以确保不存在相关。但在某些情况下，GCD测试认为可以整除，跨选代相关也不存在。例如，一种情况可能是因为GCD测试没有考虑循环边界。

一般来说，确定是否实际存在相关是一个NP完全（NP-complete）问题。然而，在实践中。许多常见情况可以以较低的成本进行精确分析。最近，使用不同层次精确测试的方法的通用性和成本都有所提高，并被证明是准确和高效的。（如果一个测试能够精确地判断是否存在相关，就说这一测试是确切的。尽管一般情况是“NP完全”的，但对于受限情况，是存在确切测试的，其成本也要低得多。）

除了检测是否存在相关以外，编译器还希望划分相关的类型。编译器可以通过这种分类来识别名称相关，并在编译时通过重命名和复制操作消除这些相关。

相关分析是检测循环级别并行的一种基本工具。针对向量计算机、SIMD计算机或多处理器进行有效的程序编译，都依赖于这种分析。相关分析的主要缺点是它仅适用于非常有限的一些情况，也就是用于分析单个循环嵌套中引用之间的相关以及使用仿射索引功能的情景。因此，在许多情况下，面向数组的相关分析不能告诉我们希望知道的内容；例如，分析用指针而不是数据索引完成的访问可能要困难得多。(这就是对于许多为并行计算机设计的科学应用程序，Fortran仍然优于C和C+\+的一个理由。）同理，分析过程调用之间的引用也极为困难。因此，尽管依然需要分析那些以顺序语言编写的代码，但我们也需要编写显式并行循环的方法，比如 OpenMP和CUDA。

==== 清除相关计算

前而曾经提到，相关计算的最重要形式之一是递推。点积是递推的一个完美示例：

[source,c]
----
for (i = 9999; i >= 0; i=i-1)
	sum = sum + x[i] * y[i];
----

这个循环不是并行的，因为它的变量求和存在跨迭代相关。但是，我们可以将它转换为一组循环，其中一个是完全并行的，而另一个是部分并行的。第一个循环将执行这个循环中完全并行的部分。它看起来如下所示：

[source,c]
----
for (i = 9999; i >= 0; i=i-1)
	sum[i] = x[i] + y[i];
----

注意，这一求和已经从标量扩展到向量值（这种转换被称为标量扩展，scalar expansion)，这一转换使新的循环成为完全并行的循环。但是，在完成转换时，需要进行归约步骤，对向量的元素求和，类似如下所示：

[source,c]
----
for (i = 9999; i >= 0; i=i-1)
	finalsum = finalsum + sum[i];
----

尽管这个循环不是并行的，但它有一种非常特殊的结构，称为归约（reduction）。归约在线性代数中很常见，它还是仓库级计算机中主要并行原型MapReduce的关健部分。一般来说，任何函数都可用作归约运算符，常见情况中包含诸如max和min之类的运算符。

在向量和SIMD体系结构中，归约有时是由特殊硬件处理的，这使得归约步骤的执行速度比在标量模式下快得多。具体做法是实现一种技术，它类似于可在多处理器环境中实现的技术。下例中的代码变换可以使用任意数量的处理器，但为简便起见，我们假定有l0个处理器。在归约求和的第一步中，每个处理器执行以下运算（p是处理器号，范围为0-9）：

[source,c]
----
for (i = 9999; i >= 0; i=i-1)
	finalsum[p] = finalsum[p] + sum[i+1000*p];
----

这个循环在10个处理器中的每一个上对1000个元素求和，它是完全并行的。然后用简单的标量循环来完成最后l0个总和的计算。向量处理器和SIMD处理器中使用了类似的方法。

以上变换依赖于加法的结合性质，注意到这一点很重要。尽管拥有无限范围与精度的算术运算具有结合性质，但计算机运算却不具备结合性：对于整数运算来说，是因为其范围有限；对于浮点运算来说，既有范围原因，又有精度原因。因此，使用这些代码变换技术有时会导致一些错误行为，尽管这种现象很少发生。为此，大多数编译器要求显式启用那些依赖结合性的优化。

<<<